{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "from helpers import *\n",
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization, gradient_descent_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109.81967768  73.68895452  96.58434842 ...  58.32779473  74.38901745\n",
      "  51.59669261]\n",
      "[[ 1.          1.94406149]\n",
      " [ 1.          0.62753668]\n",
      " [ 1.          2.01244346]\n",
      " ...\n",
      " [ 1.         -0.64968792]\n",
      " [ 1.          0.69312469]\n",
      " [ 1.         -1.14970831]]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(e):\n",
    "    return 1/2 * np.mean(e**2)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    \n",
    "    return compute_mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    \n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=18.793541019523236, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.117 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = - tx.T.dot(err) / len(y)\n",
    "    \n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.706078  ,  6.52028757])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad, err = compute_gradient(y, tx, np.array([100, 20]))\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23.293922  ,  -3.47971243])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad, err = compute_gradient(y, tx, np.array([50, 10]))\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    conv_epsilon = 0.00001\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = compute_mse(err)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        if np.linalg.norm(ws[-1]-ws[-2])/len(w)<conv_epsilon: break\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.2367127591674, w0=51.305745401473644, w1=9.435798704492269\n",
      "Gradient Descent(1/49): loss=265.3024621089598, w0=66.69746902191571, w1=12.266538315840005\n",
      "Gradient Descent(2/49): loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244333\n",
      "Gradient Descent(3/49): loss=17.410212120174467, w0=72.70024123388814, w1=13.370526764265632\n",
      "Gradient Descent(4/49): loss=15.568077051450455, w0=73.11581777164007, w1=13.446956733772023\n",
      "Gradient Descent(5/49): loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "Gradient Descent(6/49): loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "Gradient Descent(7/49): loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "Gradient Descent(8/49): loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "Gradient Descent(9/49): loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "Gradient Descent(10/49): loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "Gradient Descent(11/49): loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "Gradient Descent(12/49): loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "Gradient Descent: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b938b920a9f34434955c9bfbd69a14f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=15, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.88903289 60.19843863 79.91765052]\n",
      "[[ 1.         -0.91708013]\n",
      " [ 1.         -1.20131401]\n",
      " [ 1.          0.59330881]]\n"
     ]
    }
   ],
   "source": [
    "for minibatch_y, minibatch_tx in batch_iter(y, tx, 3):\n",
    "    print(minibatch_y)\n",
    "    print(minibatch_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = - tx.T.dot(err) / len(y)\n",
    "    \n",
    "    return grad, err\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, num_batches, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    conv_epsilon = 0.00001\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=num_batches):\n",
    "            grad, err = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if np.linalg.norm(ws[-1]-ws[-2])/len(w)<conv_epsilon: break\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def minibatch_stochastic_gradient_descent(y, tx, initial_w, batch_size, num_batches, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    conv_epsilon = 0.00001\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=num_batches):\n",
    "            grad, err = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        w = w - gamma * grad\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if np.linalg.norm(ws[-1]-ws[-2])/len(w)<conv_epsilon: break\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=252.54389129479574, w0=51.66019188557523, w1=10.970184970091031\n",
      "Gradient Descent(1/499): loss=43.07051960578972, w0=66.62158051439368, w1=10.185911811805472\n",
      "Gradient Descent(2/499): loss=18.443321775606446, w0=70.91468757076619, w1=12.805834601635638\n",
      "Gradient Descent(3/499): loss=16.025635147071828, w0=72.31057093479328, w1=12.920681814270129\n",
      "Gradient Descent(4/499): loss=15.641877144334806, w0=72.81840773173843, w1=14.014375627745371\n",
      "Gradient Descent(5/499): loss=16.113911177789042, w0=72.78777272528336, w1=14.575093431488447\n",
      "Gradient Descent(6/499): loss=16.128335510174175, w0=74.51041040005069, w1=13.408640215707946\n",
      "Gradient Descent(7/499): loss=15.681140512630291, w0=73.92920630094663, w1=13.912053905643116\n",
      "Gradient Descent(8/499): loss=15.431411520742177, w0=73.59142238238425, w1=13.530119057166731\n",
      "Gradient Descent(9/499): loss=16.297704418513693, w0=74.62326861376827, w1=13.717348049803958\n",
      "Gradient Descent(10/499): loss=16.116825786253955, w0=72.41000463293965, w1=12.654748242228797\n",
      "Gradient Descent(11/499): loss=15.661688969374081, w0=72.6809548766593, w1=13.899085185906369\n",
      "Gradient Descent(12/499): loss=15.574136234176619, w0=72.84719151901129, w1=13.900341257312398\n",
      "Gradient Descent(13/499): loss=15.556582546427864, w0=73.29321618226999, w1=12.895427524166179\n",
      "Gradient Descent(14/499): loss=15.704602957426545, w0=72.49586127821972, w1=13.456706822941294\n",
      "Gradient Descent(15/499): loss=15.672283358022215, w0=72.58621942226344, w1=13.747943747238319\n",
      "Gradient Descent(16/499): loss=15.451798210497168, w0=73.00370909942825, w1=13.26154471406003\n",
      "Gradient Descent(17/499): loss=15.387892753756354, w0=73.24883495181572, w1=13.524175090668882\n",
      "Gradient Descent(18/499): loss=15.51066466890405, w0=73.20662485298935, w1=12.987845778357862\n",
      "Gradient Descent(19/499): loss=15.495122969021159, w0=72.8936868273533, w1=13.721129098197805\n",
      "Gradient Descent(20/499): loss=15.658065506719273, w0=73.55160794897344, w1=14.171054049942438\n",
      "Gradient Descent(21/499): loss=15.429610033789137, w0=73.00690226503453, w1=13.408550644319021\n",
      "Gradient Descent(22/499): loss=15.541829957922175, w0=73.84576212153004, w1=13.565483413639853\n",
      "Gradient Descent(23/499): loss=15.511347570020781, w0=73.09699614534102, w1=13.940298593400767\n",
      "Gradient Descent(24/499): loss=15.531334759683649, w0=73.35134678101595, w1=12.94343249272424\n",
      "Gradient Descent(25/499): loss=15.39140989507077, w0=73.23084607698215, w1=13.56376884552754\n",
      "Gradient Descent(26/499): loss=15.541016218492537, w0=73.85081557793062, w1=13.468476575847713\n",
      "Gradient Descent(27/499): loss=15.532131781966172, w0=73.60662091272194, w1=13.038456027021246\n",
      "Gradient Descent(28/499): loss=15.463074446493259, w0=73.68673713835007, w1=13.471380324533914\n",
      "Gradient Descent(29/499): loss=15.466213017719399, w0=73.29335464891452, w1=13.07890078858846\n",
      "Gradient Descent(30/499): loss=15.738525399468816, w0=72.76833256596515, w1=14.134716866956365\n",
      "Gradient Descent(31/499): loss=15.437824205995426, w0=72.97358890871655, w1=13.44422463973319\n",
      "Gradient Descent(32/499): loss=15.806028344295797, w0=73.04147341309701, w1=14.360933554141656\n",
      "Gradient Descent(33/499): loss=16.158791696236612, w0=74.38372380156859, w1=14.078160170970019\n",
      "Gradient Descent(34/499): loss=15.823051063025828, w0=73.48208585694493, w1=12.563786263223715\n",
      "Gradient Descent(35/499): loss=16.07872781004077, w0=72.1190529794922, w1=13.55294263454586\n",
      "Gradient Descent(36/499): loss=15.698195953041138, w0=72.56408788616406, w1=13.782958756701124\n",
      "Gradient Descent(37/499): loss=15.543587521507279, w0=73.81305727361347, w1=13.265474540974505\n",
      "Gradient Descent(38/499): loss=15.640288781336265, w0=72.6986907492867, w1=13.086645544139841\n",
      "Gradient Descent(39/499): loss=15.788898271288502, w0=72.70091380588319, w1=12.805648577382268\n",
      "Gradient Descent(40/499): loss=16.327779642289084, w0=72.90136365342606, w1=12.16453887617573\n",
      "Gradient Descent(41/499): loss=15.713090885726091, w0=74.04075403255109, w1=13.168829897535701\n",
      "Gradient Descent(42/499): loss=15.40625018870609, w0=73.27549839638986, w1=13.278751716176064\n",
      "Gradient Descent(43/499): loss=15.409603093434614, w0=73.16130353455775, w1=13.306961772831727\n",
      "Gradient Descent(44/499): loss=15.484664879240386, w0=73.61365261577262, w1=13.78846209977562\n",
      "Gradient Descent(45/499): loss=15.44343978665797, w0=73.19849997400459, w1=13.805286500618227\n",
      "Gradient Descent(46/499): loss=15.560084932300807, w0=72.70380514495213, w1=13.492211308802718\n",
      "Gradient Descent(47/499): loss=16.284452369711516, w0=72.58801630050817, w1=12.340051896838741\n",
      "Gradient Descent(48/499): loss=15.802406698421084, w0=72.59523233251564, w1=12.892455757894435\n",
      "Gradient Descent(49/499): loss=15.440142674155751, w0=73.16583509780601, w1=13.783197781587123\n",
      "Gradient Descent(50/499): loss=15.42880978888628, w0=73.41522445952019, w1=13.746413679715891\n",
      "Gradient Descent(51/499): loss=15.504079435882174, w0=73.74791766316845, w1=13.305726591193416\n",
      "Gradient Descent(52/499): loss=15.899149254206964, w0=73.85834732401959, w1=12.63831667392476\n",
      "Gradient Descent(53/499): loss=15.527176957096966, w0=73.15741828059532, w1=13.993468126489482\n",
      "Gradient Descent(54/499): loss=15.51364165522412, w0=73.51034780916942, w1=13.022911118412534\n",
      "Gradient Descent(55/499): loss=15.461158926347661, w0=73.19167022506642, w1=13.105430871017578\n",
      "Gradient Descent(56/499): loss=15.590335946521575, w0=72.67470492403133, w1=13.320130588884835\n",
      "Gradient Descent(57/499): loss=15.57304915676088, w0=73.33963081340643, w1=12.86960319851382\n",
      "Gradient Descent(58/499): loss=15.818162017188506, w0=74.1262669334583, w1=13.065284863549838\n",
      "Gradient Descent(59/499): loss=15.431000670287613, w0=73.31530017614442, w1=13.779326481040256\n",
      "Gradient Descent(60/499): loss=15.468125630749823, w0=73.61154484679031, w1=13.731885495356282\n",
      "Gradient Descent(61/499): loss=15.457524015801205, w0=73.45443388303424, w1=13.136917700455061\n",
      "Gradient Descent(62/499): loss=15.633668139161204, w0=73.69212353080029, w1=12.89919879758809\n",
      "Gradient Descent(63/499): loss=15.520111102903245, w0=73.78198741472364, w1=13.305819878640978\n",
      "Gradient Descent(64/499): loss=15.570228983206576, w0=73.33506330095258, w1=14.085509118937964\n",
      "Gradient Descent(65/499): loss=15.735525920435306, w0=73.9930689516467, w1=13.93848214427531\n",
      "Gradient Descent(66/499): loss=15.727446378689116, w0=73.79997197383327, w1=14.133188100820185\n",
      "Gradient Descent(67/499): loss=15.430888672585663, w0=73.20405872116055, w1=13.76593996380204\n",
      "Gradient Descent(68/499): loss=15.692800567825005, w0=72.53767629705804, w1=13.274973988211325\n",
      "Gradient Descent(69/499): loss=15.787478601896634, w0=72.40547131984438, w1=13.597342581315443\n",
      "Gradient Descent(70/499): loss=15.399648272056147, w0=73.45948382870047, w1=13.46922015077962\n",
      "Gradient Descent(71/499): loss=15.555499972059089, w0=73.86822603721797, w1=13.382763573662263\n",
      "Gradient Descent(72/499): loss=15.791922408660067, w0=73.14882256300349, w1=14.3691028062174\n",
      "Gradient Descent(73/499): loss=15.743591650870643, w0=73.12942975107926, w1=14.309380961230046\n",
      "Gradient Descent(74/499): loss=15.764348503399402, w0=73.74139588304786, w1=14.225827972298191\n",
      "Gradient Descent(75/499): loss=15.4096507822858, w0=73.26454697766148, w1=13.263696382688009\n",
      "Gradient Descent(76/499): loss=15.51560331032334, w0=73.58051686184793, w1=13.058649174472386\n",
      "Gradient Descent(77/499): loss=15.859042496387486, w0=73.92545881401841, w1=12.739799933192614\n",
      "Gradient Descent(78/499): loss=15.537536125982681, w0=73.78073019656081, w1=13.222196811934023\n",
      "Gradient Descent(79/499): loss=15.711131710451394, w0=74.0293526352463, w1=13.148609026547437\n",
      "Gradient Descent(80/499): loss=15.578230088617001, w0=73.83611535480516, w1=13.178530085135726\n",
      "Gradient Descent(81/499): loss=15.618442885453572, w0=73.95084958342889, w1=13.296528958265482\n",
      "Gradient Descent(82/499): loss=15.733343161065513, w0=74.08513500663703, w1=13.217238501174817\n",
      "Gradient Descent(83/499): loss=15.505954167693824, w0=73.23667269573357, w1=13.966390064070984\n",
      "Gradient Descent(84/499): loss=15.529282375396273, w0=73.56807003331778, w1=13.939747075115207\n",
      "Gradient Descent(85/499): loss=15.443618683596995, w0=73.42507705292394, w1=13.16624795186549\n",
      "Gradient Descent(86/499): loss=15.52938916862323, w0=72.78340664557429, w1=13.317303474848961\n",
      "Gradient Descent(87/499): loss=15.674811556484425, w0=72.55746741964897, w1=13.291345709452697\n",
      "Gradient Descent(88/499): loss=15.401339897302224, w0=73.11813941088344, w1=13.481842592028674\n",
      "Gradient Descent(89/499): loss=15.472326535098851, w0=73.07021539883054, w1=13.830187381583274\n",
      "Gradient Descent(90/499): loss=15.632087791573177, w0=73.47721856359874, w1=14.157061839677791\n",
      "Gradient Descent(91/499): loss=15.826049408368236, w0=72.79646279835735, w1=14.275235797112526\n",
      "Gradient Descent(92/499): loss=15.660594288493304, w0=73.55230883123974, w1=14.17444194052167\n",
      "Gradient Descent(93/499): loss=17.31215173860463, w0=71.34336999316284, w1=13.698515085707488\n",
      "Gradient Descent(94/499): loss=15.415172983047828, w0=73.10629004880158, w1=13.632566870764839\n",
      "Gradient Descent(95/499): loss=15.486900341443869, w0=72.85934458920826, w1=13.364963067109514\n",
      "Gradient Descent(96/499): loss=16.134304581264402, w0=72.26071986929915, w1=14.134942759100396\n",
      "Gradient Descent(97/499): loss=15.759212383709524, w0=72.46373354264423, w1=13.719370840929876\n",
      "Gradient Descent(98/499): loss=15.843999310162312, w0=74.22402589042437, w1=13.253593794449703\n",
      "Gradient Descent(99/499): loss=15.395951465033018, w0=73.35194084718107, w1=13.609176739039608\n",
      "Gradient Descent(100/499): loss=15.423024308490637, w0=73.27682682232324, w1=13.207718622282409\n",
      "Gradient Descent(101/499): loss=15.521026486718888, w0=72.79267833138434, w1=13.617669016351664\n",
      "Gradient Descent(102/499): loss=15.39660707867833, w0=73.18862608152448, w1=13.377971642742681\n",
      "Gradient Descent(103/499): loss=15.729251312779681, w0=72.85049214382074, w1=14.179781609201425\n",
      "Gradient Descent(104/499): loss=16.015403239953162, w0=72.40410137655292, w1=14.163269306499592\n",
      "Gradient Descent(105/499): loss=15.700945269213838, w0=73.76354521988321, w1=12.839736783817308\n",
      "Gradient Descent(106/499): loss=15.410715702509714, w0=73.41292123392826, w1=13.291311664857524\n",
      "Gradient Descent(107/499): loss=16.092755237477128, w0=73.09077700568194, w1=14.651235737242851\n",
      "Gradient Descent(108/499): loss=15.489753114632773, w0=73.7202247989631, w1=13.640946479154268\n",
      "Gradient Descent(109/499): loss=15.90021630856234, w0=74.28632011993994, w1=13.270421118437422\n",
      "Gradient Descent(110/499): loss=15.4207319321806, w0=73.39449531108711, w1=13.235636348034955\n",
      "Gradient Descent(111/499): loss=15.400777334421965, w0=73.34456674560471, w1=13.644679222982877\n",
      "Gradient Descent(112/499): loss=16.774161464393632, w0=71.68012640429339, w1=13.89469551851381\n",
      "Gradient Descent(113/499): loss=15.829803421490503, w0=72.35684753120209, w1=13.381109487244649\n",
      "Gradient Descent(114/499): loss=15.479542342525123, w0=72.92755217329919, w1=13.249316914346585\n",
      "Gradient Descent(115/499): loss=15.96109697377058, w0=72.66149413636268, w1=12.613425412972253\n",
      "Gradient Descent(116/499): loss=15.748704417204296, w0=72.51768657478806, w1=13.830556642761653\n",
      "Gradient Descent(117/499): loss=17.36091464581257, w0=71.43586056323325, w1=14.185163527087903\n",
      "Gradient Descent(118/499): loss=17.339800557120228, w0=71.35849748940743, w1=13.077273206992707\n",
      "Gradient Descent(119/499): loss=16.82810400748667, w0=71.59845656483924, w1=13.380569970065507\n",
      "Gradient Descent(120/499): loss=16.58102811184493, w0=71.78352447245278, w1=13.149591565532162\n",
      "Gradient Descent(121/499): loss=15.622145869910574, w0=72.96056319321286, w1=12.878556958680099\n",
      "Gradient Descent(122/499): loss=15.508837110186935, w0=73.78974611065401, w1=13.487258036147548\n",
      "Gradient Descent(123/499): loss=15.722026058166149, w0=73.79257807532217, w1=12.828852619577683\n",
      "Gradient Descent(124/499): loss=15.52368022346397, w0=73.81829674020696, w1=13.504528625466996\n",
      "Gradient Descent(125/499): loss=15.398088547188198, w0=73.36258002700882, w1=13.620024345865945\n",
      "Gradient Descent(126/499): loss=15.848815466439875, w0=72.34766597655029, w1=13.654225269250585\n",
      "Gradient Descent(127/499): loss=15.492345771873396, w0=73.61423281976388, w1=13.147572726163366\n",
      "Gradient Descent(128/499): loss=15.7332782944899, w0=73.93265793888186, w1=12.944177884851497\n",
      "Gradient Descent(129/499): loss=15.529324914395675, w0=73.82508624607041, w1=13.54855019399819\n",
      "Gradient Descent(130/499): loss=15.918478057763789, w0=74.32466171480154, w1=13.427214601324875\n",
      "Gradient Descent(131/499): loss=15.488423576339326, w0=73.39316365844691, w1=13.92155234792857\n",
      "Gradient Descent(132/499): loss=16.129536944322396, w0=72.1957768244834, w1=14.010160660409781\n",
      "Gradient Descent(133/499): loss=15.434907525650145, w0=73.31348447659636, w1=13.792213032161712\n",
      "Gradient Descent(134/499): loss=15.475255033066697, w0=73.50227392847377, w1=13.8475762729143\n",
      "Gradient Descent(135/499): loss=16.314471896823054, w0=72.10469482671904, w1=14.145224855091321\n",
      "Gradient Descent(136/499): loss=15.496644680351132, w0=72.82369370953641, w1=13.499686816364179\n",
      "Gradient Descent(137/499): loss=15.493035938450182, w0=73.73957807119554, w1=13.354465449697969\n",
      "Gradient Descent(138/499): loss=15.451127064176054, w0=72.93721818564313, w1=13.422784603353625\n",
      "Gradient Descent(139/499): loss=15.676752040543484, w0=73.72335181828731, w1=12.849380477682459\n",
      "Gradient Descent(140/499): loss=15.544580491168457, w0=72.80772929476076, w1=13.19510411465528\n",
      "Gradient Descent(141/499): loss=16.37476505028912, w0=71.91616253206713, w1=13.76172875657328\n",
      "Gradient Descent(142/499): loss=15.687019799370875, w0=72.54341567956541, w1=13.28221782516192\n",
      "Gradient Descent(143/499): loss=15.91972714198216, w0=74.32114527757857, w1=13.591475079156345\n",
      "Gradient Descent(144/499): loss=16.42927670394757, w0=74.5853038780728, w1=14.127099892898684\n",
      "Gradient Descent(145/499): loss=15.502021855417633, w0=73.16751453768165, w1=13.944781382688715\n",
      "Gradient Descent(146/499): loss=15.532428039192459, w0=72.85314250983002, w1=13.794027213381282\n",
      "Gradient Descent(147/499): loss=15.678588909826997, w0=72.70653669566158, w1=13.969998661360284\n",
      "Gradient Descent(148/499): loss=15.480836465391098, w0=73.18874916805372, w1=13.902602335673594\n",
      "Gradient Descent(149/499): loss=15.386111269731726, w0=73.2869406241576, w1=13.459760939641503\n",
      "Gradient Descent(150/499): loss=15.55078905504651, w0=72.82051972165819, w1=13.804816500330892\n",
      "Gradient Descent(151/499): loss=15.836967137773335, w0=72.95851781573903, w1=14.368342037048265\n",
      "Gradient Descent(152/499): loss=15.64348847507218, w0=72.5784912762084, w1=13.421746160929496\n",
      "Gradient Descent(153/499): loss=15.638397503305049, w0=73.34492352461821, w1=12.770897851580486\n",
      "Gradient Descent(154/499): loss=15.71965810494005, w0=72.48263970082633, w1=13.576467279069174\n",
      "Gradient Descent(155/499): loss=15.542801944096604, w0=73.29840394325274, w1=14.039898174487975\n",
      "Gradient Descent(156/499): loss=15.870839203460305, w0=72.35211103413351, w1=13.191797916650588\n",
      "Gradient Descent(157/499): loss=15.645428626978653, w0=72.64685304394311, w1=13.79654563981699\n",
      "Gradient Descent(158/499): loss=16.375770134661877, w0=73.1776862252146, w1=12.077480722383891\n",
      "Gradient Descent(159/499): loss=15.502512588501816, w0=73.67176053095902, w1=13.178901056450266\n",
      "Gradient Descent(160/499): loss=15.665983506007407, w0=73.6877402077848, w1=12.843238951463375\n",
      "Gradient Descent(161/499): loss=15.718677910079048, w0=73.02501582417399, w1=12.70947250485706\n",
      "Gradient Descent(162/499): loss=15.844821515189151, w0=73.11170035243094, w1=12.53914741166025\n",
      "Gradient Descent(163/499): loss=15.965850655440622, w0=73.3032499029272, w1=12.40275442118874\n",
      "Gradient Descent(164/499): loss=15.732935703752696, w0=73.00690687070362, w1=14.261836455968542\n",
      "Gradient Descent(165/499): loss=15.5972965332494, w0=72.64395977472948, w1=13.460570025924032\n",
      "Gradient Descent(166/499): loss=15.603174080765125, w0=73.3325834409098, w1=14.13779884042606\n",
      "Gradient Descent(167/499): loss=15.838174278633696, w0=73.74377664058979, w1=12.641736526121445\n",
      "Gradient Descent(168/499): loss=15.420917942698571, w0=73.11299909363872, w1=13.286510341495273\n",
      "Gradient Descent(169/499): loss=15.751370247466927, w0=72.8654140618694, w1=14.21954059235394\n",
      "Gradient Descent(170/499): loss=15.587416382317471, w0=72.84922476476285, w1=13.026610466248856\n",
      "Gradient Descent(171/499): loss=15.613623203934502, w0=73.15758039027266, w1=12.818742212442594\n",
      "Gradient Descent(172/499): loss=15.750578381888795, w0=72.46489323865902, w1=13.274548267801654\n",
      "Gradient Descent(173/499): loss=15.437221332577298, w0=73.02764706756464, w1=13.657938658455816\n",
      "Gradient Descent(174/499): loss=15.57894324123884, w0=73.84132979054039, w1=13.773745524928541\n",
      "Gradient Descent(175/499): loss=15.44191418260097, w0=73.50709108410786, w1=13.221620261490349\n",
      "Gradient Descent(176/499): loss=15.464327640959825, w0=73.68926140871444, w1=13.503926022965356\n",
      "Gradient Descent(177/499): loss=15.4322810298678, w0=73.33676652699809, w1=13.178132149221723\n",
      "Gradient Descent(178/499): loss=15.643941765040601, w0=73.82415950534272, w1=12.99498985187286\n",
      "Gradient Descent(179/499): loss=15.652005526696449, w0=74.0227074142266, w1=13.512986130502496\n",
      "Gradient Descent(180/499): loss=15.532697021934561, w0=73.24222685067217, w1=14.019106463069301\n",
      "Gradient Descent(181/499): loss=15.57413122426055, w0=72.91879353737515, w1=13.965269172983264\n",
      "Gradient Descent(182/499): loss=16.450311796149883, w0=71.95772193939806, w1=14.065730563601477\n",
      "Gradient Descent(183/499): loss=15.625103102884532, w0=73.0647512754081, w1=14.132331195166374\n",
      "Gradient Descent(184/499): loss=15.521439694141785, w0=72.93492492683819, w1=13.856839403760194\n",
      "Gradient Descent(185/499): loss=15.796781227270538, w0=72.76119684550268, w1=14.213190876666173\n",
      "Gradient Descent(186/499): loss=15.691397266428883, w0=73.99257833603997, w1=13.830280719709416\n",
      "Gradient Descent(187/499): loss=15.841376980054367, w0=72.8817178264795, w1=14.340564303806607\n",
      "Gradient Descent(188/499): loss=15.512865883406688, w0=73.76692205971294, w1=13.305853369485868\n",
      "Gradient Descent(189/499): loss=15.786704229269102, w0=73.98301772911383, w1=14.051359004031353\n",
      "Gradient Descent(190/499): loss=15.696699930342247, w0=74.07201640073683, w1=13.606965057137904\n",
      "Gradient Descent(191/499): loss=15.549287218408523, w0=72.89588083601397, w1=13.890031736576228\n",
      "Gradient Descent(192/499): loss=15.803577475301688, w0=72.49766381151265, w1=13.03098968444159\n",
      "Gradient Descent(193/499): loss=15.592030748888497, w0=72.88749323286919, w1=12.982619470187183\n",
      "Gradient Descent(194/499): loss=15.564534622575907, w0=73.0140421825166, w1=12.951545042886815\n",
      "Gradient Descent(195/499): loss=15.47424935281754, w0=73.55451060129681, w1=13.149838898694261\n",
      "Gradient Descent(196/499): loss=16.329239153110443, w0=74.64659603004324, w1=13.241016932928515\n",
      "Gradient Descent(197/499): loss=15.790443221418155, w0=73.08527491989271, w1=14.354685063281918\n",
      "Gradient Descent(198/499): loss=15.42053270188242, w0=73.55223401077973, w1=13.530354041209323\n",
      "Gradient Descent(199/499): loss=15.952676993542296, w0=73.49648352032418, w1=12.434462665430305\n",
      "Gradient Descent(200/499): loss=15.4112028420782, w0=73.45102964494941, w1=13.640793579315552\n",
      "Gradient Descent(201/499): loss=15.526070013249605, w0=73.81711975190713, w1=13.398297427511347\n",
      "Gradient Descent(202/499): loss=15.450635384917778, w0=73.04622263057159, w1=13.740748933430577\n",
      "Gradient Descent(203/499): loss=15.614662560606464, w0=73.13958638816966, w1=14.138294170102668\n",
      "Gradient Descent(204/499): loss=15.451564768334178, w0=73.09042680424548, w1=13.179806610674824\n",
      "Gradient Descent(205/499): loss=15.433575499950384, w0=73.57694028588242, w1=13.35611666932629\n",
      "Gradient Descent(206/499): loss=15.603743852809043, w0=72.7211134306715, w1=13.80774034302642\n",
      "Gradient Descent(207/499): loss=15.546676362818573, w0=72.73259799515317, w1=13.399137332149065\n",
      "Gradient Descent(208/499): loss=15.388806456149297, w0=73.3330020799702, w1=13.41406243762531\n",
      "Gradient Descent(209/499): loss=16.151986718854936, w0=74.53058731430342, w1=13.426265211892522\n",
      "Gradient Descent(210/499): loss=15.683074543510651, w0=73.84982903598565, w1=14.013885364895392\n",
      "Gradient Descent(211/499): loss=15.939389584304141, w0=74.22698755942025, w1=12.99351073126742\n",
      "Gradient Descent(212/499): loss=15.84912512544255, w0=74.25610417640904, w1=13.453636072546871\n",
      "Gradient Descent(213/499): loss=16.576365452161223, w0=74.74548125228777, w1=14.003096529008934\n",
      "Gradient Descent(214/499): loss=15.769735395676314, w0=74.06453791634893, w1=13.896661072602985\n",
      "Gradient Descent(215/499): loss=15.929981718882607, w0=74.33223311905262, w1=13.58019986967517\n",
      "Gradient Descent(216/499): loss=15.402631043010304, w0=73.17080318100403, w1=13.615093764028456\n",
      "Gradient Descent(217/499): loss=15.435309829311763, w0=73.53063383046123, w1=13.272803200586457\n",
      "Gradient Descent(218/499): loss=15.925946751931368, w0=74.27709706606143, w1=13.142837740453269\n",
      "Gradient Descent(219/499): loss=15.577441673274857, w0=73.10036165832571, w1=12.89179917420402\n",
      "Gradient Descent(220/499): loss=15.838722570835579, w0=74.23191065325491, w1=13.318943363564623\n",
      "Gradient Descent(221/499): loss=15.997581730756577, w0=73.2072415635209, w1=12.377045565340199\n",
      "Gradient Descent(222/499): loss=15.4875267455054, w0=72.84775574111168, w1=13.414801566075761\n",
      "Gradient Descent(223/499): loss=15.791667026829586, w0=74.03261824932892, w1=12.964070921592437\n",
      "Gradient Descent(224/499): loss=15.526613437378424, w0=73.70722426450034, w1=13.14709797800154\n",
      "Gradient Descent(225/499): loss=15.63692144278185, w0=72.81715487371682, w1=14.003888219619737\n",
      "Gradient Descent(226/499): loss=15.84591950209726, w0=72.3577414692992, w1=13.68858865797962\n",
      "Gradient Descent(227/499): loss=15.784028572991968, w0=72.44362108885551, w1=13.209028550629652\n",
      "Gradient Descent(228/499): loss=16.12345718603505, w0=72.44435818634182, w1=12.611737805545955\n",
      "Gradient Descent(229/499): loss=15.624640977303523, w0=73.03986824623867, w1=12.837090535207878\n",
      "Gradient Descent(230/499): loss=15.570168140473566, w0=73.84101832601672, w1=13.74285907785348\n",
      "Gradient Descent(231/499): loss=15.511457762796466, w0=72.88113323056581, w1=13.195555404414153\n",
      "Gradient Descent(232/499): loss=15.59254902833367, w0=72.90055084528316, w1=12.9712033868994\n",
      "Gradient Descent(233/499): loss=16.55351121321717, w0=72.47887576149179, w1=12.187061548462639\n",
      "Gradient Descent(234/499): loss=15.723394942472652, w0=73.4103724660065, w1=12.666414557982309\n",
      "Gradient Descent(235/499): loss=15.41948959234386, w0=73.13104746967758, w1=13.68139370262539\n",
      "Gradient Descent(236/499): loss=15.504725576688886, w0=72.91009886877477, w1=13.780303882038668\n",
      "Gradient Descent(237/499): loss=15.479774432088943, w0=72.88185347361714, w1=13.61377456367862\n",
      "Gradient Descent(238/499): loss=15.66936438838762, w0=72.58310342357187, w1=13.231337743185883\n",
      "Gradient Descent(239/499): loss=15.554106077511554, w0=73.71202262608773, w1=13.077682230166433\n",
      "Gradient Descent(240/499): loss=16.062800781741675, w0=72.30257071242345, w1=12.870574974089712\n",
      "Gradient Descent(241/499): loss=15.389129486677785, w0=73.3005640304675, w1=13.399468312736978\n",
      "Gradient Descent(242/499): loss=15.786434146159374, w0=74.18754766208995, w1=13.529969120879997\n",
      "Gradient Descent(243/499): loss=15.88525626516328, w0=73.7506146226028, w1=14.368626740786176\n",
      "Gradient Descent(244/499): loss=15.930155716461988, w0=74.1985156288061, w1=12.95986046971696\n",
      "Gradient Descent(245/499): loss=15.393127460431264, w0=73.27014135343804, w1=13.597668630047615\n",
      "Gradient Descent(246/499): loss=16.151810021865735, w0=72.65102684459151, w1=12.42210659039564\n",
      "Gradient Descent(247/499): loss=15.47168771482904, w0=73.2976227453846, w1=13.065483034175365\n",
      "Gradient Descent(248/499): loss=15.407292991078576, w0=73.49956180697663, w1=13.456853844682833\n",
      "Gradient Descent(249/499): loss=15.386178359189312, w0=73.28867058574224, w1=13.456187908511167\n",
      "Gradient Descent(250/499): loss=15.795824203064951, w0=74.07058977878464, w1=13.014245107307602\n",
      "Gradient Descent(251/499): loss=16.00659422499822, w0=74.36204031050985, w1=13.796786545550598\n",
      "Gradient Descent(252/499): loss=15.582545130439993, w0=72.97264226584846, w1=14.018316050175578\n",
      "Gradient Descent(253/499): loss=15.499807325786067, w0=73.58451758489932, w1=13.858385276864293\n",
      "Gradient Descent(254/499): loss=15.676301294203178, w0=74.00649144483945, w1=13.750030100022121\n",
      "Gradient Descent(255/499): loss=15.45333366810524, w0=73.326139808776, w1=13.845572533230166\n",
      "Gradient Descent(256/499): loss=15.773200334546104, w0=74.17345412399061, w1=13.447336857694754\n",
      "Gradient Descent(257/499): loss=15.401247984604314, w0=73.16923860304448, w1=13.602896368797351\n",
      "Gradient Descent(258/499): loss=15.673243875154471, w0=72.66146393126309, w1=13.061730615675298\n",
      "Gradient Descent(259/499): loss=15.551833555244011, w0=73.05879957486178, w1=12.953776300999007\n",
      "Gradient Descent(260/499): loss=15.61707622092939, w0=73.88424847294478, w1=13.1422344919675\n",
      "Gradient Descent(261/499): loss=16.42380834806541, w0=74.57161748825034, w1=12.813878219019086\n",
      "Gradient Descent(262/499): loss=15.443487516805542, w0=73.26097518976165, w1=13.141905088253253\n",
      "Gradient Descent(263/499): loss=15.565211260551022, w0=73.78308073901944, w1=13.134212053594615\n",
      "Gradient Descent(264/499): loss=15.728614167615882, w0=72.83003823461954, w1=14.165470736314141\n",
      "Gradient Descent(265/499): loss=15.448503321657684, w0=73.07188182157583, w1=13.755264723832854\n",
      "Gradient Descent(266/499): loss=15.5372335157134, w0=72.74667251681211, w1=13.536363071476718\n",
      "Gradient Descent(267/499): loss=15.57232303801066, w0=73.4743640725702, w1=14.063074241734075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(268/499): loss=15.936934205157486, w0=72.94530519759473, w1=14.469944224152117\n",
      "Gradient Descent(269/499): loss=15.771143088784791, w0=73.60623318549234, w1=14.300061241681723\n",
      "Gradient Descent(270/499): loss=15.557298787070836, w0=72.81053122005711, w1=13.149326012274939\n",
      "Gradient Descent(271/499): loss=15.643003534990287, w0=73.9888261758604, w1=13.302682715395521\n",
      "Gradient Descent(272/499): loss=15.435820969980222, w0=73.50208878158371, w1=13.241946175349078\n",
      "Gradient Descent(273/499): loss=15.747524521210805, w0=74.00423492246031, w1=13.947397008114343\n",
      "Gradient Descent(274/499): loss=15.489492101517182, w0=73.69340189401669, w1=13.261482553135199\n",
      "Gradient Descent(275/499): loss=15.68938281821629, w0=72.68884414864917, w1=13.970498223323258\n",
      "Gradient Descent(276/499): loss=15.483691516454606, w0=73.6727577244788, w1=13.707946504978768\n",
      "Gradient Descent(277/499): loss=15.970041731193366, w0=74.0708446901645, w1=14.231176912863166\n",
      "Gradient Descent(278/499): loss=15.7667324677241, w0=73.43271734555435, w1=14.341353340718479\n",
      "Gradient Descent(279/499): loss=15.419045427332733, w0=73.21904676647692, w1=13.72610402595002\n",
      "Gradient Descent(280/499): loss=15.586393430958653, w0=73.02212782316019, w1=12.907751729564172\n",
      "Gradient Descent(281/499): loss=15.464851010204391, w0=73.56804242161382, w1=13.19198986312789\n",
      "Gradient Descent(282/499): loss=15.70215290989701, w0=74.03720798991021, w1=13.196770705178956\n",
      "Gradient Descent(283/499): loss=15.833786695448996, w0=74.24026881458308, w1=13.494724555163618\n",
      "Gradient Descent(284/499): loss=16.240802532215078, w0=74.47115427068455, w1=12.910543292653623\n",
      "Gradient Descent(285/499): loss=15.393028004704847, w0=73.1814566752532, w1=13.520108251690369\n",
      "Gradient Descent(286/499): loss=15.702476881411817, w0=73.1106165017026, w1=14.25403608740255\n",
      "Gradient Descent(287/499): loss=16.330796650328608, w0=74.48914961615526, w1=14.158865226053475\n",
      "Gradient Descent(288/499): loss=15.523645823558219, w0=73.81080845954467, w1=13.38836529724901\n",
      "Gradient Descent(289/499): loss=15.512696330979798, w0=72.94150424114716, w1=13.119964403636374\n",
      "Gradient Descent(290/499): loss=15.527503429062275, w0=72.7660419406992, w1=13.412082846525468\n",
      "Gradient Descent(291/499): loss=15.55140921096686, w0=73.86915027463534, w1=13.467257766912669\n",
      "Gradient Descent(292/499): loss=15.652049776854101, w0=73.11952609068342, w1=14.18816841959838\n",
      "Gradient Descent(293/499): loss=15.465716092184826, w0=73.59553036307591, w1=13.741797999840314\n",
      "Gradient Descent(294/499): loss=15.660413001640213, w0=73.56852163911526, w1=12.791493498185094\n",
      "Gradient Descent(295/499): loss=15.608834626908553, w0=73.30792499585505, w1=12.812106688815195\n",
      "Gradient Descent(296/499): loss=15.426396793802839, w0=73.5750068172325, w1=13.4348885947729\n",
      "Gradient Descent(297/499): loss=15.51800479893182, w0=73.8007294673985, w1=13.393805198228595\n",
      "Gradient Descent(298/499): loss=15.494011280686042, w0=73.758868236767, w1=13.488187255759074\n",
      "Gradient Descent(299/499): loss=15.681659469822446, w0=73.43043434616456, w1=12.722805847684176\n",
      "Gradient Descent(300/499): loss=15.469652426771255, w0=72.96886881457773, w1=13.728448321346347\n",
      "Gradient Descent(301/499): loss=15.531105275234397, w0=73.82475304632837, w1=13.386689774852261\n",
      "Gradient Descent(302/499): loss=15.637381231033034, w0=73.95998304234628, w1=13.723329788804872\n",
      "Gradient Descent(303/499): loss=15.40247442837407, w0=73.44302579890568, w1=13.375112354353227\n",
      "Gradient Descent(304/499): loss=15.804358413530469, w0=74.11869136766305, w1=13.875561627282565\n",
      "Gradient Descent(305/499): loss=15.465652641435852, w0=73.26006430391539, w1=13.877686305345115\n",
      "Gradient Descent(306/499): loss=15.473349579541004, w0=73.408519888253, w1=13.077480231015196\n",
      "Gradient Descent(307/499): loss=15.53225229042066, w0=73.05535029586929, w1=12.994107261863588\n",
      "Gradient Descent(308/499): loss=15.524037493453632, w0=72.76979636803226, w1=13.519606907740474\n",
      "Gradient Descent(309/499): loss=15.415969086248236, w0=73.12162023184396, w1=13.305142864895858\n",
      "Gradient Descent(310/499): loss=15.527801210970054, w0=72.76318871841792, w1=13.43335657378646\n",
      "Gradient Descent(311/499): loss=15.538919139627682, w0=73.4199638787848, w1=14.018392290672967\n",
      "Gradient Descent(312/499): loss=15.49986898625964, w0=72.86091364071554, w1=13.278550823772768\n",
      "Gradient Descent(313/499): loss=15.762134547833481, w0=73.02163122434062, w1=12.656091715025953\n",
      "Gradient Descent(314/499): loss=15.445127047740215, w0=73.14005690197838, w1=13.787614836383709\n",
      "Gradient Descent(315/499): loss=16.153551562089532, w0=72.43658521704013, w1=14.374307887411305\n",
      "Gradient Descent(316/499): loss=15.544266660777364, w0=73.03499311986877, w1=12.979999099325768\n",
      "Gradient Descent(317/499): loss=15.422583418029268, w0=73.55945347218571, w1=13.533416594945358\n",
      "Gradient Descent(318/499): loss=15.386024636260254, w0=73.30537072766246, w1=13.491648160556625\n",
      "Gradient Descent(319/499): loss=15.519590969565416, w0=73.15979455268815, w1=13.979128122352703\n",
      "Gradient Descent(320/499): loss=15.39438469059421, w0=73.37770823428748, w1=13.37984496868337\n",
      "Gradient Descent(321/499): loss=15.39453284463557, w0=73.18034820545863, w1=13.413448234038421\n",
      "Gradient Descent(322/499): loss=15.474545617797236, w0=73.24603314105802, w1=13.06135594236191\n",
      "Gradient Descent(323/499): loss=15.982660056140457, w0=74.34760017604357, w1=13.191083468341567\n",
      "Gradient Descent(324/499): loss=15.84412924095132, w0=73.75477189201916, w1=12.64060569373969\n",
      "Gradient Descent(325/499): loss=15.604232735462876, w0=73.5459276363168, w1=12.868825462143372\n",
      "Gradient Descent(326/499): loss=15.884205851180683, w0=72.30577933685974, w1=13.337550422397104\n",
      "Gradient Descent(327/499): loss=15.492717588537321, w0=73.72194351371607, w1=13.305193023420646\n",
      "Gradient Descent(328/499): loss=15.656228798312878, w0=73.63111715029937, w1=12.826274380540692\n",
      "Gradient Descent(329/499): loss=15.589374039727309, w0=73.61967226290331, w1=12.931206182166187\n",
      "Gradient Descent(330/499): loss=15.799060720703583, w0=72.46714742936429, w1=13.857587427039947\n",
      "Gradient Descent(331/499): loss=16.124952200603172, w0=72.63300660222811, w1=14.50016306963673\n",
      "Gradient Descent(332/499): loss=15.467379885468661, w0=73.27841180497886, w1=13.883127193102368\n",
      "Gradient Descent(333/499): loss=15.950748024342337, w0=74.27175912160709, w1=13.896311617357549\n",
      "Gradient Descent(334/499): loss=15.589734445764185, w0=73.5132794315256, w1=12.880066312657187\n",
      "Gradient Descent(335/499): loss=15.610957010207814, w0=73.9111101053366, w1=13.216620952826599\n",
      "Gradient Descent(336/499): loss=15.792646980151174, w0=72.6869656164805, w1=14.146887201143286\n",
      "Gradient Descent(337/499): loss=15.781061831094767, w0=72.46201385446072, w1=13.793203676355507\n",
      "Gradient Descent(338/499): loss=15.865352563447788, w0=72.36336338639202, w1=13.784655137736257\n",
      "Gradient Descent(339/499): loss=15.513217742349406, w0=73.00014338905275, w1=13.069402949901272\n",
      "Gradient Descent(340/499): loss=16.573966833410303, w0=73.14552121501477, w1=11.945393323606822\n",
      "Gradient Descent(341/499): loss=15.65992223305355, w0=72.55422199831264, w1=13.44950262139142\n",
      "Gradient Descent(342/499): loss=15.655141684498295, w0=73.25079581595558, w1=14.212274898814476\n",
      "Gradient Descent(343/499): loss=15.409509332735892, w0=73.40005736029049, w1=13.669391673589017\n",
      "Gradient Descent(344/499): loss=15.775461755251087, w0=72.5209699727189, w1=13.905966975271017\n",
      "Gradient Descent(345/499): loss=15.41710950842101, w0=73.24857451126672, w1=13.725449864490281\n",
      "Gradient Descent(346/499): loss=16.324275296954216, w0=74.614004958876, w1=13.845985343477894\n",
      "Gradient Descent(347/499): loss=16.10761218319035, w0=73.65705937319153, w1=14.62495471435285\n",
      "Gradient Descent(348/499): loss=15.456067311618861, w0=73.4051185835851, w1=13.837475213721136\n",
      "Gradient Descent(349/499): loss=15.526838438074838, w0=72.94031180140298, w1=13.875769204215658\n",
      "Gradient Descent(350/499): loss=16.624190052694008, w0=73.07283016032333, w1=15.037827567200635\n",
      "Gradient Descent(351/499): loss=15.841210415371444, w0=72.61527370905695, w1=14.150593637160341\n",
      "Gradient Descent(352/499): loss=15.66402260569766, w0=72.61965379692126, w1=13.798509958841654\n",
      "Gradient Descent(353/499): loss=16.259916726246413, w0=71.99355393870204, w1=13.718669783725497\n",
      "Gradient Descent(354/499): loss=15.691982097270788, w0=72.9317173491407, w1=14.173251364144184\n",
      "Gradient Descent(355/499): loss=15.526041265035467, w0=73.4554308104256, w1=13.983916458423069\n",
      "Gradient Descent(356/499): loss=15.421691061770717, w0=73.3976126034991, w1=13.233025092254966\n",
      "Gradient Descent(357/499): loss=15.393906005377502, w0=73.20130727673858, w1=13.566076696935012\n",
      "Gradient Descent(358/499): loss=15.575354487023736, w0=73.3863001380073, w1=12.871107870106465\n",
      "Gradient Descent(359/499): loss=15.555258073517535, w0=73.65158165394016, w1=13.020561062535954\n",
      "Gradient Descent(360/499): loss=15.566799318562255, w0=73.24366952473156, w1=12.880298064137362\n",
      "Gradient Descent(361/499): loss=15.456921258418774, w0=73.6171994362058, w1=13.673512537620793\n",
      "Gradient Descent(362/499): loss=15.745138689377264, w0=73.39573390408772, w1=12.638204278337891\n",
      "Gradient Descent(363/499): loss=15.706022793157343, w0=73.90396528966842, w1=12.961912245770337\n",
      "Gradient Descent(364/499): loss=15.60501460029923, w0=73.80366227337464, w1=13.057316508415644\n",
      "Gradient Descent(365/499): loss=15.976307632050238, w0=74.1120089685769, w1=14.194955919217177\n",
      "Gradient Descent(366/499): loss=15.514450273531429, w0=73.57842256046948, w1=13.899455501274206\n",
      "Gradient Descent(367/499): loss=15.659666126695164, w0=73.25567636001608, w1=14.218694022440808\n",
      "Gradient Descent(368/499): loss=15.85405704473624, w0=74.20745636749072, w1=13.798763081553727\n",
      "Gradient Descent(369/499): loss=15.529134105121432, w0=73.8152162513785, w1=13.601140513532028\n",
      "Gradient Descent(370/499): loss=16.060750347156493, w0=74.28745136447982, w1=12.877529437260248\n",
      "Gradient Descent(371/499): loss=15.471074328288847, w0=73.03935469001422, w1=13.154799519681468\n",
      "Gradient Descent(372/499): loss=15.59822526095427, w0=73.57822622287961, w1=14.066096178197874\n",
      "Gradient Descent(373/499): loss=15.577895854961575, w0=73.62575737406236, w1=14.003068201349178\n",
      "Gradient Descent(374/499): loss=15.445774978200818, w0=73.6399098459624, w1=13.4715496752922\n",
      "Gradient Descent(375/499): loss=15.622374328512878, w0=73.2344941602613, w1=14.164870264237706\n",
      "Gradient Descent(376/499): loss=15.409488850838926, w0=73.11046890087894, w1=13.596103687536349\n",
      "Gradient Descent(377/499): loss=15.404880308453768, w0=73.2084539099485, w1=13.654869748487021\n",
      "Gradient Descent(378/499): loss=15.547026817652451, w0=73.20076629280004, w1=12.919712514033732\n",
      "Gradient Descent(379/499): loss=15.540577900011234, w0=72.78466782349898, w1=13.256015667163078\n",
      "Gradient Descent(380/499): loss=15.685788907133887, w0=73.34652819576425, w1=14.252392617857536\n",
      "Gradient Descent(381/499): loss=15.412876425231284, w0=73.21433257458258, w1=13.261440504062525\n",
      "Gradient Descent(382/499): loss=15.62647763663035, w0=73.87064699568751, w1=13.094267474742619\n",
      "Gradient Descent(383/499): loss=16.213377305670093, w0=74.5679070464357, w1=13.300992037430294\n",
      "Gradient Descent(384/499): loss=15.526180953674048, w0=73.39959010893695, w1=12.960655192589051\n",
      "Gradient Descent(385/499): loss=15.61395871294337, w0=73.6491020172988, w1=12.905265879568093\n",
      "Gradient Descent(386/499): loss=15.579753972237146, w0=73.76383343976421, w1=13.071159565004367\n",
      "Gradient Descent(387/499): loss=16.390916727115744, w0=71.88346144544937, w1=13.623443902284317\n",
      "Gradient Descent(388/499): loss=15.494681012948575, w0=73.5658813334764, w1=13.100734321744396\n",
      "Gradient Descent(389/499): loss=15.797443565708466, w0=72.41738647890585, w1=13.245625121518627\n",
      "Gradient Descent(390/499): loss=15.52788660615065, w0=72.77217992106041, w1=13.588260468179737\n",
      "Gradient Descent(391/499): loss=15.504069748811592, w0=73.7675306141712, w1=13.58952428563043\n",
      "Gradient Descent(392/499): loss=15.500809194590449, w0=72.83846799727169, w1=13.330031771564457\n",
      "Gradient Descent(393/499): loss=15.74103613345691, w0=72.45659815048134, w1=13.57555238885395\n",
      "Gradient Descent(394/499): loss=15.880596100932625, w0=73.08876776781018, w1=12.506404567787062\n",
      "Gradient Descent(395/499): loss=16.078008400731306, w0=72.20670395931978, w1=13.030048125480635\n",
      "Gradient Descent(396/499): loss=15.984709708514691, w0=72.2697854597506, w1=13.093981930641059\n",
      "Gradient Descent(397/499): loss=15.47842063043254, w0=72.8756601778774, w1=13.379101453481415\n",
      "Gradient Descent(398/499): loss=15.786536598852477, w0=72.43795549328122, w1=13.217760538989289\n",
      "Gradient Descent(399/499): loss=15.929370661295426, w0=72.32169205305404, w1=13.856188876615944\n",
      "Gradient Descent(400/499): loss=15.390030842160183, w0=73.37541637167563, w1=13.520266280551214\n",
      "Gradient Descent(401/499): loss=15.390428899146425, w0=73.27788251912601, w1=13.573652818317028\n",
      "Gradient Descent(402/499): loss=15.52737502810341, w0=73.63550516946708, w1=13.887505591286503\n",
      "Gradient Descent(403/499): loss=15.421023844383656, w0=73.34247285223903, w1=13.740317051333037\n",
      "Gradient Descent(404/499): loss=15.48436011133942, w0=72.8534084189184, w1=13.533492245773187\n",
      "Gradient Descent(405/499): loss=15.522215513405332, w0=73.28514017467214, w1=14.001802627062029\n",
      "Gradient Descent(406/499): loss=15.524508202032104, w0=72.98609129786362, w1=13.9068902948221\n",
      "Gradient Descent(407/499): loss=15.565667753571208, w0=72.72675931672254, w1=13.674356360640866\n",
      "Gradient Descent(408/499): loss=15.47435576659616, w0=72.88391157733366, w1=13.38575900443866\n",
      "Gradient Descent(409/499): loss=15.501768528191315, w0=72.99450760092184, w1=13.102734537642482\n",
      "Gradient Descent(410/499): loss=15.858042725789899, w0=73.14634353130828, w1=12.519228021331939\n",
      "Gradient Descent(411/499): loss=15.38757387035398, w0=73.24513108597098, w1=13.448225143077137\n",
      "Gradient Descent(412/499): loss=15.523824818685007, w0=72.80165404188485, w1=13.29655634021817\n",
      "Gradient Descent(413/499): loss=15.466384995281283, w0=73.23785428510618, w1=13.877016681145921\n",
      "Gradient Descent(414/499): loss=15.703901212657755, w0=72.84799700391294, w1=14.140906493979255\n",
      "Gradient Descent(415/499): loss=15.651526385243056, w0=72.60737875565553, w1=13.724529516871984\n",
      "Gradient Descent(416/499): loss=16.475783119176008, w0=71.84498754002568, w1=13.196199780436217\n",
      "Gradient Descent(417/499): loss=15.435292838855666, w0=73.00627618829873, w1=13.352945618147452\n",
      "Gradient Descent(418/499): loss=15.695140910782822, w0=73.11933480198935, w1=14.246540573141628\n",
      "Gradient Descent(419/499): loss=15.538797331709427, w0=73.69045852299394, w1=13.094254637867884\n",
      "Gradient Descent(420/499): loss=15.556568345374574, w0=72.85994795664051, w1=13.870899705939122\n",
      "Gradient Descent(421/499): loss=15.406185612304288, w0=73.12211692424214, w1=13.374457971092063\n",
      "Gradient Descent(422/499): loss=15.747202626249706, w0=74.08373243223504, w1=13.794083234044425\n",
      "Gradient Descent(423/499): loss=15.513454379988485, w0=73.47779533660943, w1=13.009261992140432\n",
      "Gradient Descent(424/499): loss=15.760923645743189, w0=74.00313306496018, w1=13.976795146568998\n",
      "Gradient Descent(425/499): loss=15.457209603513478, w0=73.0141290361249, w1=13.733403912094815\n",
      "Gradient Descent(426/499): loss=15.450332017637201, w0=73.08345381801078, w1=13.770558513009025\n",
      "Gradient Descent(427/499): loss=15.511951052259768, w0=73.16355326571956, w1=13.964614857579354\n",
      "Gradient Descent(428/499): loss=15.88259644139928, w0=72.6754880255016, w1=12.698075331390443\n",
      "Gradient Descent(429/499): loss=15.546655009037963, w0=73.36384276604024, w1=12.916999926519699\n",
      "Gradient Descent(430/499): loss=15.542450522981701, w0=73.84500669334209, w1=13.576825623531791\n",
      "Gradient Descent(431/499): loss=15.413584937134315, w0=73.33613654064443, w1=13.711255237394452\n",
      "Gradient Descent(432/499): loss=15.453507122887924, w0=73.56526983612952, w1=13.727923758436138\n",
      "Gradient Descent(433/499): loss=16.293601322101225, w0=74.46380254994001, w1=14.14814822872046\n",
      "Gradient Descent(434/499): loss=15.408400882229119, w0=73.49865462858787, w1=13.423939874953325\n",
      "Gradient Descent(435/499): loss=15.482455464671602, w0=73.62936346425457, w1=13.19578600586437\n",
      "Gradient Descent(436/499): loss=15.50054889312429, w0=73.66661361697729, w1=13.179008246801036\n",
      "Gradient Descent(437/499): loss=15.630338154333353, w0=73.99298193301888, w1=13.465022846879004\n",
      "Gradient Descent(438/499): loss=15.428490809761934, w0=73.55934859411654, w1=13.358243895365408\n",
      "Gradient Descent(439/499): loss=15.887164511980908, w0=73.63394527065833, w1=12.537938861806273\n",
      "Gradient Descent(440/499): loss=16.047679657974637, w0=74.39290544163951, w1=13.82003430898451\n",
      "Gradient Descent(441/499): loss=15.924236991472478, w0=74.00189283516052, w1=12.72111139041483\n",
      "Gradient Descent(442/499): loss=15.747118512018773, w0=73.16322475789555, w1=12.639843685281841\n",
      "Gradient Descent(443/499): loss=15.850217480177944, w0=72.61828835194211, w1=12.792560054898283\n",
      "Gradient Descent(444/499): loss=15.601482221740353, w0=72.91130736571112, w1=12.946050795146963\n",
      "Gradient Descent(445/499): loss=15.455719209843883, w0=72.92524343429112, w1=13.540858127098412\n",
      "Gradient Descent(446/499): loss=15.607137107536238, w0=72.64439653564435, w1=13.33613268073205\n",
      "Gradient Descent(447/499): loss=15.787106110680003, w0=73.64316569114088, w1=14.304615659234145\n",
      "Gradient Descent(448/499): loss=16.35140826721157, w0=72.08988850737398, w1=12.78592274039154\n",
      "Gradient Descent(449/499): loss=15.779293550195549, w0=73.69452230612833, w1=12.688301821496367\n",
      "Gradient Descent(450/499): loss=15.53999601697218, w0=72.74425535727576, w1=13.40171964142759\n",
      "Gradient Descent(451/499): loss=15.779023326470261, w0=73.85589596773309, w1=12.793814354194115\n",
      "Gradient Descent(452/499): loss=15.868048473514285, w0=73.93621778618821, w1=12.736895273690964\n",
      "Gradient Descent(453/499): loss=15.554207753098392, w0=72.97373434947713, w1=12.995853327766174\n",
      "Gradient Descent(454/499): loss=15.77129216491555, w0=74.16612226156877, w1=13.580088226711716\n",
      "Gradient Descent(455/499): loss=15.66133124574851, w0=72.56471712168579, w1=13.618085106462676\n",
      "Gradient Descent(456/499): loss=15.455095642323956, w0=72.96807317643405, w1=13.300162748835472\n",
      "Gradient Descent(457/499): loss=15.492681337880764, w0=72.83369040837489, w1=13.437595713426179\n",
      "Gradient Descent(458/499): loss=15.546595795215978, w0=72.97049677468436, w1=13.0140817067924\n",
      "Gradient Descent(459/499): loss=15.449249793678304, w0=72.99254758297522, w1=13.290246582678174\n",
      "Gradient Descent(460/499): loss=15.394976864691472, w0=73.22031824775623, w1=13.59267472546636\n",
      "Gradient Descent(461/499): loss=15.541667769177508, w0=72.84974998830299, w1=13.817752407257284\n",
      "Gradient Descent(462/499): loss=15.398765976248711, w0=73.42724330086959, w1=13.390372376032316\n",
      "Gradient Descent(463/499): loss=15.854555362872002, w0=72.41075053400697, w1=13.08304740499216\n",
      "Gradient Descent(464/499): loss=15.481053268296359, w0=72.86763163684132, w1=13.57248809676041\n",
      "Gradient Descent(465/499): loss=15.393969388472518, w0=73.1746373157591, w1=13.523692008538054\n",
      "Gradient Descent(466/499): loss=16.16530917402335, w0=72.82502882283906, w1=14.636856244904477\n",
      "Gradient Descent(467/499): loss=15.512025752785313, w0=73.78728455191175, w1=13.57388866532446\n",
      "Gradient Descent(468/499): loss=15.44966302802939, w0=72.93798581040986, w1=13.509033857897997\n",
      "Gradient Descent(469/499): loss=15.605906403129993, w0=72.63190426464797, w1=13.521778857385186\n",
      "Gradient Descent(470/499): loss=15.557075750951713, w0=73.74449491813591, w1=13.106403164297577\n",
      "Gradient Descent(471/499): loss=15.391259636515903, w0=73.37236155612389, w1=13.547467669018276\n",
      "Gradient Descent(472/499): loss=15.385959380582353, w0=73.30569044380618, w1=13.477584693032666\n",
      "Gradient Descent(473/499): loss=15.783694593235309, w0=73.40505423777486, w1=14.364733946030942\n",
      "Gradient Descent(474/499): loss=15.752054009078853, w0=73.77323342305672, w1=14.18865026177242\n",
      "Gradient Descent(475/499): loss=16.485830569151485, w0=74.69217017118548, w1=13.974472435704325\n",
      "Gradient Descent(476/499): loss=15.739720936052908, w0=73.93624625784334, w1=12.93649453677743\n",
      "Gradient Descent(477/499): loss=15.601917501965461, w0=72.8149233944339, w1=13.029579565581475\n",
      "Gradient Descent(478/499): loss=15.386718841720558, w0=73.28998488123788, w1=13.520288842686103\n",
      "Gradient Descent(479/499): loss=15.580316396518231, w0=73.88735065477962, w1=13.28814132661759\n",
      "Gradient Descent(480/499): loss=15.518857288678744, w0=73.80106989610688, w1=13.386225252400548\n",
      "Gradient Descent(481/499): loss=15.607757240099916, w0=73.76885695955568, w1=13.012619800993889\n",
      "Gradient Descent(482/499): loss=15.540108137121663, w0=73.58158788016624, w1=13.954779671476565\n",
      "Gradient Descent(483/499): loss=15.428224537724937, w0=73.13080758980492, w1=13.72068344214761\n",
      "Gradient Descent(484/499): loss=16.276125747730383, w0=72.43007555008651, w1=12.46273403544621\n",
      "Gradient Descent(485/499): loss=15.830769037290954, w0=72.82570384377486, w1=12.660850973487246\n",
      "Gradient Descent(486/499): loss=16.14380099085198, w0=74.4134310154862, w1=12.967339706398002\n",
      "Gradient Descent(487/499): loss=15.765276914207428, w0=73.79758069653273, w1=12.769004330150219\n",
      "Gradient Descent(488/499): loss=15.423335125363003, w0=73.15324608915182, w1=13.244968317742327\n",
      "Gradient Descent(489/499): loss=15.760105336826612, w0=74.14927644454293, w1=13.609341574446326\n",
      "Gradient Descent(490/499): loss=15.89430288629801, w0=73.29677890190409, w1=12.471336575965857\n",
      "Gradient Descent(491/499): loss=15.388842111554307, w0=73.23685821546171, w1=13.428212826216523\n",
      "Gradient Descent(492/499): loss=15.617605943638091, w0=73.92837641903002, w1=13.726499271068207\n",
      "Gradient Descent(493/499): loss=15.739030292097455, w0=72.50129117445981, w1=13.759035218883076\n",
      "Gradient Descent(494/499): loss=15.521248288200368, w0=73.63406043700343, w1=13.085978154227373\n",
      "Gradient Descent(495/499): loss=15.405714175545965, w0=73.1174258389788, w1=13.571917195358537\n",
      "Gradient Descent(496/499): loss=15.667084004882508, w0=73.8041033957495, w1=14.029355244170949\n",
      "Gradient Descent(497/499): loss=15.540519411699846, w0=73.66230920897924, w1=13.063114365439482\n",
      "Gradient Descent(498/499): loss=15.709617224973842, w0=74.02531115665629, w1=13.144259581529854\n",
      "Gradient Descent(499/499): loss=15.433979343165175, w0=73.00321754785554, w1=13.587758114916397\n",
      "SGD: execution time=0.366 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "# Using 'minibatch_stochastic_gradient_descent'\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 50\n",
    "num_batches = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = minibatch_stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, num_batches, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554f5cb3e593437bb54e44a9a968eb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=79.43588634964064, w0=63.072503088934724, w1=8.619404588452609\n",
      "Gradient Descent(1/499): loss=47.293760095084934, w0=66.78177024169256, w1=18.106549789150705\n",
      "Gradient Descent(2/499): loss=86.89203279862664, w0=80.24635465586614, w1=3.7495546739600556\n",
      "Gradient Descent(3/499): loss=628.0926399117714, w0=41.95466372411134, w1=-2.1172229906347617\n",
      "Gradient Descent(4/499): loss=159.40478872518185, w0=70.82427716786081, w1=-3.3113166015350366\n",
      "Gradient Descent(5/499): loss=34.20349686324773, w0=70.51707185036881, w1=8.009399717818455\n",
      "Gradient Descent(6/499): loss=102.32268165021203, w0=68.1722148815941, w1=1.3289221835497873\n",
      "Gradient Descent(7/499): loss=39.80873105769171, w0=72.59885449650045, w1=20.434032498113623\n",
      "Gradient Descent(8/499): loss=15.729436811837996, w0=74.11931185490636, w1=13.556063446181163\n",
      "Gradient Descent(9/499): loss=103.55092737199875, w0=73.7772885874889, w1=26.74984565777446\n",
      "Gradient Descent(10/499): loss=79.26399140126509, w0=80.9398418300707, w1=5.15528350267213\n",
      "Gradient Descent(11/499): loss=108.0703436797367, w0=59.691412510704026, w1=14.063362294199061\n",
      "Gradient Descent(12/499): loss=20.119970338896813, w0=73.27714038309071, w1=16.556705011742864\n",
      "Gradient Descent(13/499): loss=175.81537806206723, w0=72.97253537684206, w1=31.389366005719488\n",
      "Gradient Descent(14/499): loss=189.74283311815498, w0=70.39099332387025, w1=-4.96715438807151\n",
      "Gradient Descent(15/499): loss=22.519401115363387, w0=70.2051409566352, w1=11.305670543439271\n",
      "Gradient Descent(16/499): loss=105.18948511015498, w0=85.32283786790913, w1=7.571042739020143\n",
      "Gradient Descent(17/499): loss=58.690314057399554, w0=80.77516146116056, w1=19.015045174166815\n",
      "Gradient Descent(18/499): loss=40.29930467444293, w0=72.35359065787358, w1=20.475611989319614\n",
      "Gradient Descent(19/499): loss=17.50401584630708, w0=74.72140271882462, w1=11.996960008469525\n",
      "Gradient Descent(20/499): loss=18.25333530239605, w0=75.58414314362584, w1=12.779868173695753\n",
      "Gradient Descent(21/499): loss=371.353951369409, w0=59.03378870863243, w1=36.03153554208059\n",
      "Gradient Descent(22/499): loss=98.2754865468385, w0=68.85284816395321, w1=25.565077986766955\n",
      "Gradient Descent(23/499): loss=62.738067652622235, w0=80.8023887858548, w1=19.67061593326089\n",
      "Gradient Descent(24/499): loss=47.36552138015874, w0=75.7401558288352, w1=5.865567237554179\n",
      "Gradient Descent(25/499): loss=170.64951271379047, w0=76.12799585542133, w1=30.87210373359638\n",
      "Gradient Descent(26/499): loss=35.163770928986494, w0=75.91995045572557, w1=19.19458257184171\n",
      "Gradient Descent(27/499): loss=39.61956555935859, w0=72.64505918401846, w1=20.411258613466323\n",
      "Gradient Descent(28/499): loss=32.54143288092191, w0=70.43874114859945, w1=18.594302576726626\n",
      "Gradient Descent(29/499): loss=48.89723150242312, w0=68.95706656193389, w1=6.53605546202127\n",
      "Gradient Descent(30/499): loss=25.260536917765428, w0=69.39676660930708, w1=11.343950757681238\n",
      "Gradient Descent(31/499): loss=87.78119843357814, w0=64.50935194613712, w1=21.7029690928641\n",
      "Gradient Descent(32/499): loss=32.29800106670159, w0=69.49875278799158, w1=17.886629388142193\n",
      "Gradient Descent(33/499): loss=36.45694817739156, w0=69.01807761616227, w1=8.595116709207353\n",
      "Gradient Descent(34/499): loss=47.13946373506902, w0=76.52955592351495, w1=6.197005187055648\n",
      "Gradient Descent(35/499): loss=58.52669150896999, w0=64.00530013659211, w1=13.535489847922513\n",
      "Gradient Descent(36/499): loss=39.44983329555636, w0=79.67166057773693, w1=10.749814723653936\n",
      "Gradient Descent(37/499): loss=16.781130246600025, w0=74.20722207784209, w1=12.081010276843784\n",
      "Gradient Descent(38/499): loss=32.69991159361759, w0=71.41768500752322, w1=19.057147525578483\n",
      "Gradient Descent(39/499): loss=22.501747828022662, w0=70.68154741999344, w1=16.201333084734884\n",
      "Gradient Descent(40/499): loss=111.40974128093231, w0=74.2304707401486, w1=27.30615750541388\n",
      "Gradient Descent(41/499): loss=249.06963837550296, w0=83.91307052598104, w1=32.31056970812489\n",
      "Gradient Descent(42/499): loss=48.1875315059215, w0=81.33619394893759, w1=12.517865665811781\n",
      "Gradient Descent(43/499): loss=18.844125180788858, w0=70.68235703755752, w1=13.169546253940808\n",
      "Gradient Descent(44/499): loss=127.32742086115358, w0=78.34318015859131, w1=27.564742007333116\n",
      "Gradient Descent(45/499): loss=121.52308495955626, w0=81.91173113124017, w1=25.22738284214857\n",
      "Gradient Descent(46/499): loss=109.59499581259251, w0=83.27974208919431, w1=22.8978658142781\n",
      "Gradient Descent(47/499): loss=121.95857896836489, w0=78.21546356119511, w1=27.224668269084038\n",
      "Gradient Descent(48/499): loss=138.63200849933295, w0=59.10638007289515, w1=6.756179489586433\n",
      "Gradient Descent(49/499): loss=174.69977212956564, w0=64.94821055410573, w1=-2.2992884506248275\n",
      "Gradient Descent(50/499): loss=29.011940271155822, w0=70.79660541184943, w1=8.895444265184167\n",
      "Gradient Descent(51/499): loss=98.59002793117791, w0=67.77798361136561, w1=25.140874632397253\n",
      "Gradient Descent(52/499): loss=143.36675619766018, w0=81.30613429538792, w1=27.327679463583205\n",
      "Gradient Descent(53/499): loss=27.85968058339553, w0=77.99519053802551, w1=11.792804143973253\n",
      "Gradient Descent(54/499): loss=61.93046683302368, w0=64.6518009412252, w1=17.769572747106807\n",
      "Gradient Descent(55/499): loss=825.5951474294847, w0=64.24993151710322, w1=52.705020021144485\n",
      "Gradient Descent(56/499): loss=80.9923776620633, w0=82.1189124596544, w1=20.78262438917783\n",
      "Gradient Descent(57/499): loss=21.875366506525733, w0=76.7315021940048, w1=12.401751536045094\n",
      "Gradient Descent(58/499): loss=193.46350297299207, w0=81.42063256159898, w1=-3.5529563427021644\n",
      "Gradient Descent(59/499): loss=44.910590658554106, w0=66.57653566762848, w1=17.211484991407094\n",
      "Gradient Descent(60/499): loss=29.6779520855558, w0=74.94499496488841, w1=18.564797144708155\n",
      "Gradient Descent(61/499): loss=30.772539908773858, w0=68.00374948213945, w1=11.8101679495415\n",
      "Gradient Descent(62/499): loss=109.16763144964473, w0=71.21861247416003, w1=27.016941749913524\n",
      "Gradient Descent(63/499): loss=255.81880363799502, w0=53.799314113302565, w1=23.520932221457485\n",
      "Gradient Descent(64/499): loss=239.8050395196625, w0=64.17101743576546, w1=32.60066731635215\n",
      "Gradient Descent(65/499): loss=106.34717607860905, w0=83.17450093839966, w1=22.661037846018413\n",
      "Gradient Descent(66/499): loss=83.63551123030791, w0=78.80638192897257, w1=3.17864296247125\n",
      "Gradient Descent(67/499): loss=48.868990336463696, w0=68.01634438267382, w1=19.733781462921634\n",
      "Gradient Descent(68/499): loss=19.341404057132976, w0=75.14836160762476, w1=11.364981689459597\n",
      "Gradient Descent(69/499): loss=16.121996659718036, w0=74.03163808648611, w1=14.443036139993216\n",
      "Gradient Descent(70/499): loss=18.33504333139123, w0=74.87173203848808, w1=11.633411697239891\n",
      "Gradient Descent(71/499): loss=5304.417129716261, w0=23.833110987896703, w1=-76.69617490476284\n",
      "Gradient Descent(72/499): loss=27.98469846594222, w0=72.7465431163197, w1=18.46950176417138\n",
      "Gradient Descent(73/499): loss=35.18887817467737, w0=79.36969829881541, w1=11.839309129691785\n",
      "Gradient Descent(74/499): loss=19.237948690180808, w0=73.05985836876243, w1=16.24545570909263\n",
      "Gradient Descent(75/499): loss=26.14331776102574, w0=68.80795135138233, w1=14.659088161908073\n",
      "Gradient Descent(76/499): loss=121.55382778459997, w0=71.85167118176186, w1=27.979912171639576\n",
      "Gradient Descent(77/499): loss=21.099970620810705, w0=72.77784311169316, w1=16.820644652624374\n",
      "Gradient Descent(78/499): loss=4006.258506310223, w0=25.852630594766367, w1=-62.22411247819981\n",
      "Gradient Descent(79/499): loss=104.61998706741693, w0=68.48402411668992, w1=1.0164431717077278\n",
      "Gradient Descent(80/499): loss=41.52259730465044, w0=78.88935237014016, w1=18.058421627339463\n",
      "Gradient Descent(81/499): loss=16.048716500495562, w0=73.8901724957995, w1=12.494754257145134\n",
      "Gradient Descent(82/499): loss=20.05234979286773, w0=73.685528526551, w1=16.509492654425458\n",
      "Gradient Descent(83/499): loss=72.91273136793936, w0=75.27416295192144, w1=24.021644552911777\n",
      "Gradient Descent(84/499): loss=59.82644381757604, w0=70.0583181979327, w1=4.6246585153385364\n",
      "Gradient Descent(85/499): loss=37.876595423699186, w0=68.32988665024378, w1=17.989675642733616\n",
      "Gradient Descent(86/499): loss=16.949122232089795, w0=72.0855399595177, w1=12.188867143843401\n",
      "Gradient Descent(87/499): loss=32.34942175045713, w0=78.79797455126594, w1=11.573807584443642\n",
      "Gradient Descent(88/499): loss=23.267315608758953, w0=77.15324250248338, w1=14.411646314199237\n",
      "Gradient Descent(89/499): loss=254.72328373512258, w0=92.5617301759969, w1=23.84438126263128\n",
      "Gradient Descent(90/499): loss=39.98600941945754, w0=77.84350708868214, w1=18.818393792895034\n",
      "Gradient Descent(91/499): loss=4834.083372757885, w0=47.367080875286874, w1=108.16442017051953\n",
      "Gradient Descent(92/499): loss=39.261991607562194, w0=67.52964318982902, w1=17.290919232292172\n",
      "Gradient Descent(93/499): loss=50.14732761289265, w0=78.03118242653012, w1=6.618135126577832\n",
      "Gradient Descent(94/499): loss=548.4870859610587, w0=92.8199740037949, w1=-12.691563602143084\n",
      "Gradient Descent(95/499): loss=433.0119840037483, w0=73.66831397327235, w1=-15.418592410135744\n",
      "Gradient Descent(96/499): loss=59.188535215429205, w0=81.52653796172585, w1=9.026698943588118\n",
      "Gradient Descent(97/499): loss=55.7031897817896, w0=68.35956928026216, w1=5.977261699238712\n",
      "Gradient Descent(98/499): loss=28.186983129318897, w0=69.42452021670994, w1=16.740068296572105\n",
      "Gradient Descent(99/499): loss=94.77930775630587, w0=66.86894211841971, w1=24.319752714858385\n",
      "Gradient Descent(100/499): loss=43.27869850634964, w0=79.46896110575756, w1=17.681439858728563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(101/499): loss=36.95683017538612, w0=75.83605382139326, w1=7.423356234559906\n",
      "Gradient Descent(102/499): loss=44.62237423251562, w0=67.73710152932688, w1=18.732780011242394\n",
      "Gradient Descent(103/499): loss=60.72797245047932, w0=81.91289502889983, w1=17.52909178578917\n",
      "Gradient Descent(104/499): loss=65.19284348265543, w0=79.83896001260676, w1=21.01472329554803\n",
      "Gradient Descent(105/499): loss=190.97395452318597, w0=55.43275589311882, w1=7.8091852517449745\n",
      "Gradient Descent(106/499): loss=44.44507940291723, w0=73.64989296125434, w1=21.094938481825615\n",
      "Gradient Descent(107/499): loss=21.90423695952655, w0=75.94367849964223, w1=15.932361754633938\n",
      "Gradient Descent(108/499): loss=35.359287872821625, w0=67.07731688191717, w1=12.339264808994516\n",
      "Gradient Descent(109/499): loss=84.51679053560008, w0=76.37724659908284, w1=2.1326901488025154\n",
      "Gradient Descent(110/499): loss=29.15068963499522, w0=70.53604609271295, w1=17.943312327533803\n",
      "Gradient Descent(111/499): loss=43.48268526931089, w0=78.98459758684695, w1=18.359241735053462\n",
      "Gradient Descent(112/499): loss=40.25204769606329, w0=71.10581777420803, w1=20.183779822950356\n",
      "Gradient Descent(113/499): loss=16.221203522190144, w0=72.04962048881751, w1=13.829491008877552\n",
      "Gradient Descent(114/499): loss=1835.269505158551, w0=93.80255991819953, w1=70.21738074631449\n",
      "Gradient Descent(115/499): loss=18.468744306577253, w0=70.81691153179467, w1=13.65329816443645\n",
      "Gradient Descent(116/499): loss=39.28258416079772, w0=68.0968556076632, w1=18.03864799887875\n",
      "Gradient Descent(117/499): loss=81.49499715412239, w0=75.51030190122432, w1=24.76270311624757\n",
      "Gradient Descent(118/499): loss=30.934203191023045, w0=68.35994555102032, w1=10.881153791331522\n",
      "Gradient Descent(119/499): loss=44.80006818314532, w0=77.25077805476586, w1=6.90919869501754\n",
      "Gradient Descent(120/499): loss=81.38190999176265, w0=80.05303825431557, w1=22.769835778356033\n",
      "Gradient Descent(121/499): loss=18.391270143622414, w0=71.96558571586769, w1=11.419060290581653\n",
      "Gradient Descent(122/499): loss=121.7818239025707, w0=84.51572973196717, w1=4.159685431729228\n",
      "Gradient Descent(123/499): loss=32.60008509940642, w0=79.1398411183233, w1=13.983323542679969\n",
      "Gradient Descent(124/499): loss=24.619291866031503, w0=69.25220341575273, w1=14.939616152066748\n",
      "Gradient Descent(125/499): loss=32.2659473632644, w0=78.93114985840171, w1=14.887469901694365\n",
      "Gradient Descent(126/499): loss=937.2802574985634, w0=89.44432688934465, w1=-26.306631468889556\n",
      "Gradient Descent(127/499): loss=100.32669653472705, w0=69.05501785153467, w1=25.805024152510484\n",
      "Gradient Descent(128/499): loss=81.58181589646277, w0=65.8266055926198, w1=4.725796642847963\n",
      "Gradient Descent(129/499): loss=119.94604363843182, w0=80.79938964715794, w1=25.840467548179802\n",
      "Gradient Descent(130/499): loss=66.72613586617811, w0=71.93937300549061, w1=3.4375168080124534\n",
      "Gradient Descent(131/499): loss=20.815289350492133, w0=71.97761469683739, w1=16.500662292471638\n",
      "Gradient Descent(132/499): loss=23.937078209683815, w0=69.67961527733156, w1=11.469944393406887\n",
      "Gradient Descent(133/499): loss=119.3767827663225, w0=82.03759473967678, w1=2.0110605153764674\n",
      "Gradient Descent(134/499): loss=21.274124008473592, w0=74.50370264663488, w1=16.691083337175485\n",
      "Gradient Descent(135/499): loss=109.04414366680074, w0=64.28487285527031, w1=3.1766281848309146\n",
      "Gradient Descent(136/499): loss=180.61630203930272, w0=82.9138465078264, w1=28.90429932881448\n",
      "Gradient Descent(137/499): loss=326.09439978761895, w0=61.50362714772329, w1=-8.48402975718566\n",
      "Gradient Descent(138/499): loss=43.144767506418304, w0=66.21507903312792, w1=15.805167550818824\n",
      "Gradient Descent(139/499): loss=284.0771544549427, w0=94.73156993259812, w1=22.30069788903323\n",
      "Gradient Descent(140/499): loss=22.922383922512218, w0=73.57629804805485, w1=17.351825020650715\n",
      "Gradient Descent(141/499): loss=43.86736564308526, w0=79.19247888933396, w1=8.771211378746443\n",
      "Gradient Descent(142/499): loss=26.776286711454926, w0=75.28808478298878, w1=17.816083324176194\n",
      "Gradient Descent(143/499): loss=28.680714765463, w0=77.77318706745424, w1=16.034284467171886\n",
      "Gradient Descent(144/499): loss=237.7348716005795, w0=76.94687322325707, w1=-7.28934904692161\n",
      "Gradient Descent(145/499): loss=69.85675605248997, w0=68.83339992229928, w1=22.916103648948173\n",
      "Gradient Descent(146/499): loss=21.07951541956407, w0=76.01487113049242, w1=15.475631006022972\n",
      "Gradient Descent(147/499): loss=245.2894303661189, w0=82.63445879103182, w1=32.781563576506215\n",
      "Gradient Descent(148/499): loss=137.7671337185007, w0=88.69072218588602, w1=10.70463841794569\n",
      "Gradient Descent(149/499): loss=23.3251186270123, w0=76.58056767955, w1=15.73280402373975\n",
      "Gradient Descent(150/499): loss=29.911218871390542, w0=72.06957997815863, w1=18.728678867870926\n",
      "Gradient Descent(151/499): loss=121.81557042326811, w0=78.84250966092652, w1=-0.013713375428880958\n",
      "Gradient Descent(152/499): loss=106.61245082118185, w0=78.62050190206585, w1=1.0667888279559428\n",
      "Gradient Descent(153/499): loss=28.993828626331158, w0=72.41252416834865, w1=8.337823737861036\n",
      "Gradient Descent(154/499): loss=20.740056490535665, w0=75.65415689394848, w1=15.746346172216574\n",
      "Gradient Descent(155/499): loss=19.38526645066406, w0=76.00088587855929, w1=14.298921648341732\n",
      "Gradient Descent(156/499): loss=32.31641926796102, w0=73.2213804287793, w1=19.29828618792908\n",
      "Gradient Descent(157/499): loss=21.121803703453757, w0=71.6238364097782, w1=16.4263453920352\n",
      "Gradient Descent(158/499): loss=2044.0398670000006, w0=93.13989940458512, w1=74.00611278394918\n",
      "Gradient Descent(159/499): loss=15.861109130120672, w0=74.18341083638262, w1=13.08064835958471\n",
      "Gradient Descent(160/499): loss=148.51021563216145, w0=77.27677825511944, w1=-2.3438621167514393\n",
      "Gradient Descent(161/499): loss=77.95071598480885, w0=75.93345126808487, w1=2.6094529401105753\n",
      "Gradient Descent(162/499): loss=20.55136559548252, w0=70.08129260760461, w1=13.57955143987732\n",
      "Gradient Descent(163/499): loss=77.13506445027151, w0=84.34491108953269, w1=14.65188706039375\n",
      "Gradient Descent(164/499): loss=377.97702049839313, w0=52.624185962398, w1=30.740774896725185\n",
      "Gradient Descent(165/499): loss=26.51902434792773, w0=68.61734027662547, w1=14.108883574748155\n",
      "Gradient Descent(166/499): loss=5359.027152452224, w0=141.88374292317474, w1=90.82775051729863\n",
      "Gradient Descent(167/499): loss=24.663062331165484, w0=77.1024730751514, w1=15.491996621845066\n",
      "Gradient Descent(168/499): loss=20.362610817498833, w0=72.65180177009591, w1=10.390840876477231\n",
      "Gradient Descent(169/499): loss=67.42117394900622, w0=76.34355348032258, w1=23.214717314670377\n",
      "Gradient Descent(170/499): loss=78.29516334588993, w0=68.46656116080831, w1=23.604688398517773\n",
      "Gradient Descent(171/499): loss=41.430245647449304, w0=76.87521172067301, w1=19.74573824956405\n",
      "Gradient Descent(172/499): loss=208.18140207495065, w0=74.19181732062651, w1=33.09564505133997\n",
      "Gradient Descent(173/499): loss=375.6308862097309, w0=77.01196588772265, w1=-13.103480494875965\n",
      "Gradient Descent(174/499): loss=37.279042759819774, w0=71.82433656471544, w1=19.931582584348277\n",
      "Gradient Descent(175/499): loss=19.05181572979142, w0=70.69550086236991, w1=12.718093565408687\n",
      "Gradient Descent(176/499): loss=160.51598616332305, w0=67.6424366539128, w1=29.552077225154612\n",
      "Gradient Descent(177/499): loss=19.102724280929536, w0=70.91203706672458, w1=12.152950595468788\n",
      "Gradient Descent(178/499): loss=32.777817486281755, w0=68.17513498638854, w1=16.409196813277376\n",
      "Gradient Descent(179/499): loss=17.593824309750804, w0=73.15590366398204, w1=11.382851967484227\n",
      "Gradient Descent(180/499): loss=29.60080027404657, w0=72.63820775094707, w1=18.771202171468037\n",
      "Gradient Descent(181/499): loss=42.27727448517734, w0=70.80669407911668, w1=20.37873196618347\n",
      "Gradient Descent(182/499): loss=95.45003061289789, w0=84.68062702954789, w1=18.999787987565135\n",
      "Gradient Descent(183/499): loss=7715.720556556439, w0=32.74944910224409, w1=-103.80973745138246\n",
      "Gradient Descent(184/499): loss=50.054479535205246, w0=81.55220180448201, w1=12.412942526148946\n",
      "Gradient Descent(185/499): loss=19.54612575417591, w0=72.65812327885355, w1=10.666131841829397\n",
      "Gradient Descent(186/499): loss=15.468788249944982, w0=73.57121645201836, w1=13.777887803318688\n",
      "Gradient Descent(187/499): loss=32.036003163941366, w0=67.61343664842639, w1=12.463682545239465\n",
      "Gradient Descent(188/499): loss=38.487118948910144, w0=66.66318641095297, w1=11.984450691885584\n",
      "Gradient Descent(189/499): loss=66.16535180225824, w0=80.96513086377955, w1=6.944310192783421\n",
      "Gradient Descent(190/499): loss=37.47542526102223, w0=79.31265411885433, w1=10.65943966605373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(191/499): loss=41.756689001698774, w0=70.1032909580657, w1=20.00362811551023\n",
      "Gradient Descent(192/499): loss=77.0671559041476, w0=62.31556596313792, w1=11.795006302659925\n",
      "Gradient Descent(193/499): loss=89.33014980371482, w0=80.37369627058177, w1=23.367347145939277\n",
      "Gradient Descent(194/499): loss=4687.576340296687, w0=-16.093589075258258, w1=-23.320474633148926\n",
      "Gradient Descent(195/499): loss=22.902885211442086, w0=77.17122219988576, w1=13.45652059340395\n",
      "Gradient Descent(196/499): loss=24.143739105416667, w0=74.86781308635808, w1=17.35767186335905\n",
      "Gradient Descent(197/499): loss=37.5049176257865, w0=66.64920008040276, w1=13.772509458815149\n",
      "Gradient Descent(198/499): loss=21.951348971169736, w0=75.22051018488511, w1=16.548780726613305\n",
      "Gradient Descent(199/499): loss=47.9564483978811, w0=66.3892003461825, w1=17.658939625790673\n",
      "Gradient Descent(200/499): loss=150.38964504497756, w0=86.71586179614023, w1=22.95911360371248\n",
      "Gradient Descent(201/499): loss=37.39019868833625, w0=78.78589126449796, w1=17.200854095816638\n",
      "Gradient Descent(202/499): loss=105.9406922331108, w0=61.573714055051965, w1=20.093813614015044\n",
      "Gradient Descent(203/499): loss=19.788708653095995, w0=75.64478576501762, w1=15.290535766172525\n",
      "Gradient Descent(204/499): loss=23.32261560166129, w0=71.50805220975313, w1=9.91823457454569\n",
      "Gradient Descent(205/499): loss=200.67853685752493, w0=60.631702950074896, w1=-1.0204084836720253\n",
      "Gradient Descent(206/499): loss=61.3333537496268, w0=82.05076158851317, w1=9.579367293913343\n",
      "Gradient Descent(207/499): loss=51.874181944451635, w0=64.75435881893252, w1=13.708729447121907\n",
      "Gradient Descent(208/499): loss=18.687620527074152, w0=70.98086586282867, w1=12.360231924642985\n",
      "Gradient Descent(209/499): loss=21.733283419379383, w0=69.76671992723357, w1=13.98333592939859\n",
      "Gradient Descent(210/499): loss=63.74844813944056, w0=70.87667622836103, w1=23.012919839138938\n",
      "Gradient Descent(211/499): loss=1876.2666540358473, w0=33.51701557618946, w1=-32.77565749670627\n",
      "Gradient Descent(212/499): loss=72.85809869338875, w0=66.34802591392578, w1=21.646654897216425\n",
      "Gradient Descent(213/499): loss=407.1116301128848, w0=77.25127174414622, w1=-14.229327019804005\n",
      "Gradient Descent(214/499): loss=22.97701283871973, w0=69.65797792736682, w1=12.078940995690127\n",
      "Gradient Descent(215/499): loss=18.307944711432565, w0=74.95014585550996, w1=11.71873650312076\n",
      "Gradient Descent(216/499): loss=16.920663138386992, w0=73.98223427967852, w1=11.868570992515114\n",
      "Gradient Descent(217/499): loss=91.88821761784847, w0=84.39942448784423, w1=8.032467806988395\n",
      "Gradient Descent(218/499): loss=241.2501440481118, w0=70.31705997113822, w1=34.52411322966961\n",
      "Gradient Descent(219/499): loss=119.87101373475217, w0=81.66669177292911, w1=25.263895821911657\n",
      "Gradient Descent(220/499): loss=159.34907766675457, w0=80.39522323856222, w1=28.890679320455526\n",
      "Gradient Descent(221/499): loss=37.245073820100444, w0=70.96108467715108, w1=19.66649204297922\n",
      "Gradient Descent(222/499): loss=23.213491249709485, w0=73.76510180390349, w1=9.551201730391508\n",
      "Gradient Descent(223/499): loss=27.660010713043384, w0=70.08984069508935, w1=9.700544713590636\n",
      "Gradient Descent(224/499): loss=19.466417207729148, w0=70.57035700190569, w1=12.617591580649119\n",
      "Gradient Descent(225/499): loss=611.1431077163189, w0=81.4314339468454, w1=47.025134463101764\n",
      "Gradient Descent(226/499): loss=75.12808622144541, w0=71.62223677962335, w1=24.282020763405146\n",
      "Gradient Descent(227/499): loss=40.983568650278336, w0=66.17172948743973, w1=12.794340020209146\n",
      "Gradient Descent(228/499): loss=160.38726082610472, w0=58.98068122848612, w1=4.252909929735159\n",
      "Gradient Descent(229/499): loss=36.19199906416685, w0=66.85070450788531, w1=13.791434607530253\n",
      "Gradient Descent(230/499): loss=25.724764852213585, w0=69.25271609297894, w1=11.394908183551639\n",
      "Gradient Descent(231/499): loss=17.798707309323408, w0=73.19708279131892, w1=11.285114283107408\n",
      "Gradient Descent(232/499): loss=85.12543514507382, w0=78.9631933737303, w1=3.1192849984707767\n",
      "Gradient Descent(233/499): loss=48.04169865747992, w0=67.45512165385085, w1=19.067201322720816\n",
      "Gradient Descent(234/499): loss=3238.9982963523685, w0=6.991335572621168, w1=58.76979799163016\n",
      "Gradient Descent(235/499): loss=36.9635047446309, w0=73.90979364233625, w1=20.020043915454096\n",
      "Gradient Descent(236/499): loss=22.402136660805063, w0=69.62139252597743, w1=14.217970405433027\n",
      "Gradient Descent(237/499): loss=20.994736973051914, w0=73.90599769503197, w1=16.772592869285628\n",
      "Gradient Descent(238/499): loss=23.729620303852492, w0=75.0267886485577, w1=9.780437093025625\n",
      "Gradient Descent(239/499): loss=38.85258135165113, w0=78.30027626303249, w1=18.156228657535337\n",
      "Gradient Descent(240/499): loss=18.540813030907668, w0=72.93451676112899, w1=15.965809415582399\n",
      "Gradient Descent(241/499): loss=17.119896865243064, w0=71.96292555372925, w1=12.177227714449891\n",
      "Gradient Descent(242/499): loss=220.77871009586318, w0=90.68884134677063, w1=23.881751919948332\n",
      "Gradient Descent(243/499): loss=70.12323106447461, w0=80.81468124998577, w1=20.753835524867945\n",
      "Gradient Descent(244/499): loss=41.09834226121743, w0=66.3933009854609, w1=15.43069638403197\n",
      "Gradient Descent(245/499): loss=439.648171159185, w0=88.26810735862765, w1=38.465675283941266\n",
      "Gradient Descent(246/499): loss=1588.9071155136348, w0=59.58039404370221, w1=-40.91680956985711\n",
      "Gradient Descent(247/499): loss=39.623145130176425, w0=80.25628595665454, w1=13.478073201626781\n",
      "Gradient Descent(248/499): loss=21.970339976285807, w0=76.19824652221345, w1=11.303981914724485\n",
      "Gradient Descent(249/499): loss=140.95238349999622, w0=85.35276563240869, w1=23.761603387640832\n",
      "Gradient Descent(250/499): loss=126.60360595415202, w0=86.67166717735716, w1=20.07299463709961\n",
      "Gradient Descent(251/499): loss=22.469745451197166, w0=69.67870579234354, w1=12.431892354339647\n",
      "Gradient Descent(252/499): loss=22.339336746763035, w0=71.15492830522054, w1=16.534479810838636\n",
      "Gradient Descent(253/499): loss=24.17641759344966, w0=69.18940652927044, w1=12.622967325577457\n",
      "Gradient Descent(254/499): loss=60.95993967002196, w0=69.6609594074277, w1=4.650800475519787\n",
      "Gradient Descent(255/499): loss=41.51795898336845, w0=78.12928272451268, w1=18.854042989883965\n",
      "Gradient Descent(256/499): loss=936.554898065112, w0=55.58127719931346, w1=-25.61760504949704\n",
      "Gradient Descent(257/499): loss=173.5227836797907, w0=78.14444768806365, w1=30.589539792147857\n",
      "Gradient Descent(258/499): loss=56.43188732378102, w0=71.30495819388919, w1=22.319170669491324\n",
      "Gradient Descent(259/499): loss=74.77869791284155, w0=62.448594289566216, w1=14.558826375061379\n",
      "Gradient Descent(260/499): loss=71.6646322842462, w0=63.08729544559792, w1=16.374926272831434\n",
      "Gradient Descent(261/499): loss=51.13358821289564, w0=64.98854555851094, w1=11.893483315875997\n",
      "Gradient Descent(262/499): loss=92.46286942544424, w0=68.84408293839672, w1=1.8886427940834283\n",
      "Gradient Descent(263/499): loss=50.10939125371868, w0=69.75158510939559, w1=21.022845478249454\n",
      "Gradient Descent(264/499): loss=38.81671848314337, w0=66.53162934007283, w1=12.415259908264064\n",
      "Gradient Descent(265/499): loss=71.05059086247157, w0=65.13586449105892, w1=20.171162474684916\n",
      "Gradient Descent(266/499): loss=47.90012988498082, w0=72.44085215608037, w1=21.49848764508853\n",
      "Gradient Descent(267/499): loss=124.29611129311876, w0=65.4633680274652, w1=0.9696016721369206\n",
      "Gradient Descent(268/499): loss=41.28472296439626, w0=78.72105821179673, w1=8.752782908056309\n",
      "Gradient Descent(269/499): loss=20.139105948367824, w0=70.9411432936889, w1=11.487008616480185\n",
      "Gradient Descent(270/499): loss=53.52979578112486, w0=66.77559418284778, w1=19.29342194487048\n",
      "Gradient Descent(271/499): loss=20.294119252961128, w0=76.13514576369653, w1=12.159140473741871\n",
      "Gradient Descent(272/499): loss=44.89354920369013, w0=65.72923416282943, w1=14.817927875056471\n",
      "Gradient Descent(273/499): loss=30.353457207569246, w0=76.343660984371, w1=8.937220737965491\n",
      "Gradient Descent(274/499): loss=314.67861310443857, w0=85.25812308240005, w1=34.820831032229684\n",
      "Gradient Descent(275/499): loss=163.96966342673042, w0=77.56973950403706, w1=30.179560811725146\n",
      "Gradient Descent(276/499): loss=1005271.7870896677, w0=1269.4739712852481, w1=774.8377701848087\n",
      "Gradient Descent(277/499): loss=22.912220131856227, w0=77.16417304545718, w1=13.208011519483573\n",
      "Gradient Descent(278/499): loss=7399.202518668911, w0=119.52664225911035, w1=-98.9043115152044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(279/499): loss=212.96737146741506, w0=53.76931554191318, w1=17.215045053618823\n",
      "Gradient Descent(280/499): loss=186.36831238254297, w0=87.77350038605918, w1=1.977249270502973\n",
      "Gradient Descent(281/499): loss=35.63120708044085, w0=66.98545651773767, w1=12.6467048385251\n",
      "Gradient Descent(282/499): loss=121.81881319058216, w0=76.6799731627654, w1=27.671277119015162\n",
      "Gradient Descent(283/499): loss=23.97521080766815, w0=70.04660639050394, w1=16.05528755451639\n",
      "Gradient Descent(284/499): loss=61.87030177957483, w0=72.13450154152967, w1=23.05178493874129\n",
      "Gradient Descent(285/499): loss=22.42195799752048, w0=76.51228897748913, w1=11.552472369729886\n",
      "Gradient Descent(286/499): loss=18.768963550219752, w0=73.87841196162282, w1=16.01437668297216\n",
      "Gradient Descent(287/499): loss=774.5555745732125, w0=106.29377255369123, w1=34.200456557287225\n",
      "Gradient Descent(288/499): loss=23.89365729534246, w0=77.05545537427591, w1=11.786666203171357\n",
      "Gradient Descent(289/499): loss=54.093345197309404, w0=64.89142192708852, w1=16.08986705613791\n",
      "Gradient Descent(290/499): loss=39.454124067857585, w0=78.72335029222263, w1=9.160247093013416\n",
      "Gradient Descent(291/499): loss=16.748085189143346, w0=72.26677196337857, w1=14.77174859492104\n",
      "Gradient Descent(292/499): loss=142.48298731348072, w0=61.71852019038178, w1=24.44348330939411\n",
      "Gradient Descent(293/499): loss=33.40969664196854, w0=69.3150229318669, w1=17.975930816599885\n",
      "Gradient Descent(294/499): loss=78.09272870156501, w0=65.28149346241074, w1=5.655731954328251\n",
      "Gradient Descent(295/499): loss=26.123674892641628, w0=75.23182969760019, w1=9.27018896892241\n",
      "Gradient Descent(296/499): loss=273.4876427252995, w0=70.42843039239698, w1=36.01840067427289\n",
      "Gradient Descent(297/499): loss=15.65023511799715, w0=72.75287852623161, w1=12.993948649234962\n",
      "Gradient Descent(298/499): loss=89.77771373243475, w0=70.58747393895773, w1=1.5860674676673785\n",
      "Gradient Descent(299/499): loss=61.81751162262261, w0=82.39811466214458, w1=10.32108559465617\n",
      "Gradient Descent(300/499): loss=26.378264225540903, w0=74.46623073654088, w1=18.019585224461107\n",
      "Gradient Descent(301/499): loss=838.7247331244259, w0=52.33141756902366, w1=-21.265806677613533\n",
      "Gradient Descent(302/499): loss=23.973134426662547, w0=77.43457937907438, w1=13.65132129902858\n",
      "Gradient Descent(303/499): loss=30.31101164721893, w0=78.74647077426346, w1=13.8260640540331\n",
      "Gradient Descent(304/499): loss=45.71692512224189, w0=66.98620557208348, w1=18.04860594931602\n",
      "Gradient Descent(305/499): loss=30.22208507887774, w0=72.00814358715196, w1=8.186398889794786\n",
      "Gradient Descent(306/499): loss=46.78462443806062, w0=67.87638195289331, w1=19.263113938745096\n",
      "Gradient Descent(307/499): loss=100.10871994743724, w0=69.961504113973, w1=0.8963641057059235\n",
      "Gradient Descent(308/499): loss=54.786937346249886, w0=69.21922924923406, w1=21.366345298546246\n",
      "Gradient Descent(309/499): loss=33.831248979226324, w0=72.10834921047042, w1=7.5227719974381415\n",
      "Gradient Descent(310/499): loss=88.82925672895087, w0=74.36033199887389, w1=1.4070363671474766\n",
      "Gradient Descent(311/499): loss=93.09031941527722, w0=84.93077424278542, w1=9.008411391503936\n",
      "Gradient Descent(312/499): loss=43.435338043713415, w0=75.06474061139347, w1=6.202137165521457\n",
      "Gradient Descent(313/499): loss=100.56387308294761, w0=71.96381709534718, w1=0.4956148011426853\n",
      "Gradient Descent(314/499): loss=22.85456244061382, w0=73.08520279784527, w1=9.620465690177058\n",
      "Gradient Descent(315/499): loss=45.726064502861696, w0=72.42621970619805, w1=21.22099436976846\n",
      "Gradient Descent(316/499): loss=42.99701042698844, w0=77.91522775684489, w1=19.299143527246084\n",
      "Gradient Descent(317/499): loss=38.743723743840704, w0=68.4686157421447, w1=18.32038297197137\n",
      "Gradient Descent(318/499): loss=23.212971456868342, w0=72.56043520056467, w1=9.591761266565795\n",
      "Gradient Descent(319/499): loss=16.133657426931734, w0=72.88988018752113, w1=12.325464052589364\n",
      "Gradient Descent(320/499): loss=24.89169569523879, w0=77.61282704530606, w1=12.880817719172619\n",
      "Gradient Descent(321/499): loss=21.24684343623119, w0=76.40850866223445, w1=12.058001820500854\n",
      "Gradient Descent(322/499): loss=254.46880549036229, w0=80.23087185092153, w1=-7.257803376652234\n",
      "Gradient Descent(323/499): loss=32.34699509671476, w0=76.7117362825117, w1=8.763701408663936\n",
      "Gradient Descent(324/499): loss=23.846020936599082, w0=69.25254907436653, w1=12.713180638857663\n",
      "Gradient Descent(325/499): loss=50.54826804725563, w0=66.8870272128345, w1=18.890484978357364\n",
      "Gradient Descent(326/499): loss=21.143978039916583, w0=76.37104374303301, w1=14.910621982683695\n",
      "Gradient Descent(327/499): loss=21.797352372899837, w0=74.95679794533704, w1=16.651111590813123\n",
      "Gradient Descent(328/499): loss=95.87354037066564, w0=69.63779047254772, w1=1.3303088594118366\n",
      "Gradient Descent(329/499): loss=59.42103975926458, w0=68.15311136695276, w1=5.628446944994835\n",
      "Gradient Descent(330/499): loss=44.7363217441699, w0=66.44708778607168, w1=10.041438308507303\n",
      "Gradient Descent(331/499): loss=54.148538995354905, w0=67.5068013736207, w1=20.11556483573006\n",
      "Gradient Descent(332/499): loss=61.18531165298496, w0=67.80145523727607, w1=5.641877503991918\n",
      "Gradient Descent(333/499): loss=55.724063738558456, w0=71.89310802061328, w1=22.351807561210073\n",
      "Gradient Descent(334/499): loss=42.74080169687043, w0=77.6935540479276, w1=19.425556825607977\n",
      "Gradient Descent(335/499): loss=34.51590972332026, w0=67.86555765444267, w1=16.444995594825244\n",
      "Gradient Descent(336/499): loss=72.49965443370527, w0=63.05370485238792, w1=16.540020565943852\n",
      "Gradient Descent(337/499): loss=72.22551916575978, w0=69.01872802542663, w1=3.7123187675037377\n",
      "Gradient Descent(338/499): loss=71.98543970174757, w0=67.81481372678331, w1=4.359488963322591\n",
      "Gradient Descent(339/499): loss=38.703668333679296, w0=70.62100788182555, w1=19.763906180833416\n",
      "Gradient Descent(340/499): loss=1118.0580064612961, w0=31.990561884085118, w1=35.82645000775524\n",
      "Gradient Descent(341/499): loss=16.288716516789354, w0=71.98310492621408, w1=13.184050652757733\n",
      "Gradient Descent(342/499): loss=42.34266996350067, w0=65.95628042122635, w1=13.749119598777376\n",
      "Gradient Descent(343/499): loss=27.59314779623544, w0=69.65291748983462, w1=10.139412149619861\n",
      "Gradient Descent(344/499): loss=24.679851484139032, w0=69.4510802463878, w1=15.434320956185598\n",
      "Gradient Descent(345/499): loss=56.630229784449234, w0=67.63377564963508, w1=6.376795632726817\n",
      "Gradient Descent(346/499): loss=9727.99025665321, w0=-3.4955418019180513, w1=-102.83274169722924\n",
      "Gradient Descent(347/499): loss=27.909669238872873, w0=72.84710258122554, w1=8.494944120893425\n",
      "Gradient Descent(348/499): loss=17.00443692377966, w0=73.81807008680511, w1=15.200865215890829\n",
      "Gradient Descent(349/499): loss=40.726795324590675, w0=79.05908080432485, w1=9.303020095743594\n",
      "Gradient Descent(350/499): loss=156.25878315872941, w0=89.93648077207978, w1=11.29544409805287\n",
      "Gradient Descent(351/499): loss=291.1266483083953, w0=83.23136833706774, w1=-7.797709429956683\n",
      "Gradient Descent(352/499): loss=18.822036719709775, w0=70.8342906085413, w1=12.572788452752564\n",
      "Gradient Descent(353/499): loss=592.4321504505339, w0=52.0694970113659, w1=40.005479322600216\n",
      "Gradient Descent(354/499): loss=51.02352174650734, w0=74.59112887535814, w1=21.82192569546982\n",
      "Gradient Descent(355/499): loss=37.448200754448045, w0=76.07105890166994, w1=7.4454654718367195\n",
      "Gradient Descent(356/499): loss=29.312795136926148, w0=74.816940156618, w1=8.426571803934154\n",
      "Gradient Descent(357/499): loss=182.09945910324387, w0=83.63487181145913, w1=-1.569935385117942\n",
      "Gradient Descent(358/499): loss=27.696910862636322, w0=77.58734953360579, w1=10.992037613118939\n",
      "Gradient Descent(359/499): loss=258.41810664803387, w0=81.8329296711207, w1=-6.846373897281183\n",
      "Gradient Descent(360/499): loss=49.39087231253968, w0=66.78214664307659, w1=18.540023745666325\n",
      "Gradient Descent(361/499): loss=292.23507464453604, w0=70.72609017452092, w1=36.8699799250512\n",
      "Gradient Descent(362/499): loss=139.17136536417294, w0=88.11903698672634, w1=18.751042502765927\n",
      "Gradient Descent(363/499): loss=33.49578155293905, w0=67.72125543800959, w1=15.752414512611681\n",
      "Gradient Descent(364/499): loss=31.67163079526124, w0=78.56491490327004, w1=11.291535182403361\n",
      "Gradient Descent(365/499): loss=82.71640828003977, w0=63.31427366307996, w1=19.401507758352697\n",
      "Gradient Descent(366/499): loss=72.82867161531208, w0=83.44404855321407, w1=16.923619879947445\n",
      "Gradient Descent(367/499): loss=27.800930816873908, w0=68.33290724558829, w1=13.94706506599754\n",
      "Gradient Descent(368/499): loss=72.09308529039664, w0=71.76551942239422, w1=2.940339569516105\n",
      "Gradient Descent(369/499): loss=30.554219498586818, w0=73.62837200817546, w1=7.9820032345170535\n",
      "Gradient Descent(370/499): loss=24.958926500647696, w0=74.72278192316384, w1=9.343961404084402\n",
      "Gradient Descent(371/499): loss=115.36788803612474, w0=87.43211090538047, w1=13.204730853979708\n",
      "Gradient Descent(372/499): loss=378.38194508218737, w0=62.85154210060079, w1=38.318166823685864\n",
      "Gradient Descent(373/499): loss=17.581406371742343, w0=74.89944198870133, w1=14.826316526200058\n",
      "Gradient Descent(374/499): loss=35.377668961063485, w0=76.38405974911869, w1=7.962955154760886\n",
      "Gradient Descent(375/499): loss=69.78583295787328, w0=71.55656711165399, w1=23.764725228501412\n",
      "Gradient Descent(376/499): loss=20.805242282392292, w0=72.77666459504894, w1=10.228381384177514\n",
      "Gradient Descent(377/499): loss=26.641060684838866, w0=68.8308472673975, w1=11.86995793680218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(378/499): loss=103.51575349360843, w0=60.40611423451021, w1=10.291587250013288\n",
      "Gradient Descent(379/499): loss=23.200276313932246, w0=71.68772222943325, w1=9.867386428040852\n",
      "Gradient Descent(380/499): loss=18.757981867320094, w0=70.69721071978506, w1=13.515468738667474\n",
      "Gradient Descent(381/499): loss=59.940559147407896, w0=76.07026287387121, w1=4.457446981073476\n",
      "Gradient Descent(382/499): loss=25.736126878262578, w0=75.22343497022077, w1=17.600083501378602\n",
      "Gradient Descent(383/499): loss=59.367319088153366, w0=81.6220624417669, w1=17.79305675881937\n",
      "Gradient Descent(384/499): loss=46.13630899211777, w0=71.62309080989581, w1=5.817521796945843\n",
      "Gradient Descent(385/499): loss=23.149092975522535, w0=69.51125875848416, w1=14.583283456149308\n",
      "Gradient Descent(386/499): loss=29.50054361850777, w0=68.50650941761839, w1=11.1753703932656\n",
      "Gradient Descent(387/499): loss=24.462620299575438, w0=77.26878591541653, w1=11.945462868540579\n",
      "Gradient Descent(388/499): loss=91.95908144132399, w0=69.44524922342545, w1=1.7181604545895723\n",
      "Gradient Descent(389/499): loss=86.53092169230709, w0=67.75230716112401, w1=2.9165418597700454\n",
      "Gradient Descent(390/499): loss=85.312570364169, w0=72.93415716289599, w1=25.300200337404412\n",
      "Gradient Descent(391/499): loss=20.742097545251543, w0=75.15994548526098, w1=16.16864823064417\n",
      "Gradient Descent(392/499): loss=32.617719783057794, w0=69.56369917933928, w1=18.0128241169326\n",
      "Gradient Descent(393/499): loss=22.2899092980473, w0=69.67704404484937, w1=14.331907659543106\n",
      "Gradient Descent(394/499): loss=24.13313357717885, w0=74.77480894563442, w1=17.391421176342828\n",
      "Gradient Descent(395/499): loss=23.353959434480345, w0=69.30544565792137, w1=13.311785119795982\n",
      "Gradient Descent(396/499): loss=32.45648864071779, w0=77.66346533616773, w1=17.358925318490854\n",
      "Gradient Descent(397/499): loss=17.936927404427124, w0=73.2158294741452, w1=11.222284556125084\n",
      "Gradient Descent(398/499): loss=92.38424296999133, w0=62.31147857900042, w1=19.257484207967497\n",
      "Gradient Descent(399/499): loss=45.71013242760799, w0=72.81981289943946, w1=5.70644351668147\n",
      "Gradient Descent(400/499): loss=15.879293198121434, w0=74.21195874810023, w1=13.100213829475264\n",
      "Gradient Descent(401/499): loss=22.01909124898739, w0=70.06945478552693, w1=11.785835836907584\n",
      "Gradient Descent(402/499): loss=152.02651913327796, w0=69.58220259193614, w1=29.588852745665818\n",
      "Gradient Descent(403/499): loss=389.531654097205, w0=69.47062543095295, w1=40.566128509947475\n",
      "Gradient Descent(404/499): loss=129.9067158297704, w0=80.04467307534225, w1=-0.06536105573334439\n",
      "Gradient Descent(405/499): loss=44.653787647246375, w0=69.76082361199863, w1=6.693475235873441\n",
      "Gradient Descent(406/499): loss=62.40487555442211, w0=65.49514385997855, w1=19.243134583424226\n",
      "Gradient Descent(407/499): loss=491.0127345259417, w0=44.14820150206283, w1=3.391071800709531\n",
      "Gradient Descent(408/499): loss=41.4110446258836, w0=67.3074101765672, w1=17.506123948583028\n",
      "Gradient Descent(409/499): loss=90.24972249294599, w0=74.88387228209045, w1=25.612301909433818\n",
      "Gradient Descent(410/499): loss=37.10231250811032, w0=78.64646475143026, w1=17.32459674992097\n",
      "Gradient Descent(411/499): loss=172.54459182294542, w0=79.25456100312375, w1=-3.217239101377348\n",
      "Gradient Descent(412/499): loss=23.52183061687017, w0=77.32474193294917, w1=13.635841167746655\n",
      "Gradient Descent(413/499): loss=42.9311058876101, w0=67.28294250674173, w1=17.833855467077157\n",
      "Gradient Descent(414/499): loss=16.460656237292962, w0=72.46679748815237, w1=14.690250239167598\n",
      "Gradient Descent(415/499): loss=236.05904417598992, w0=75.14594689649105, w1=-7.446737727542004\n",
      "Gradient Descent(416/499): loss=45.141972404136176, w0=70.75032323555634, w1=20.762725162248604\n",
      "Gradient Descent(417/499): loss=76.8893852649477, w0=67.19731878827392, w1=22.744615243186157\n",
      "Gradient Descent(418/499): loss=26.229663540979377, w0=72.81691987607618, w1=18.112208556526955\n",
      "Gradient Descent(419/499): loss=18.534270586556218, w0=75.80313665853902, w1=13.504354743957592\n",
      "Gradient Descent(420/499): loss=60.35178774379075, w0=72.86029774848188, w1=22.953031270359908\n",
      "Gradient Descent(421/499): loss=78.86307779395818, w0=83.87192000954349, w1=9.598947303614972\n",
      "Gradient Descent(422/499): loss=686.1027277557668, w0=49.42905548657879, w1=41.26283367739052\n",
      "Gradient Descent(423/499): loss=371.5719049251805, w0=59.09160593331134, w1=-9.118213407768799\n",
      "Gradient Descent(424/499): loss=33.85820271670772, w0=72.64447572124264, w1=19.523126141167904\n",
      "Gradient Descent(425/499): loss=24.724847761442533, w0=72.04031916741576, w1=17.61570079547616\n",
      "Gradient Descent(426/499): loss=31.33426770891677, w0=78.91947324100607, w1=13.97964536916669\n",
      "Gradient Descent(427/499): loss=80.85981707173019, w0=61.86416047423424, w1=14.035059013084814\n",
      "Gradient Descent(428/499): loss=28.501305080098263, w0=77.61655533290755, w1=16.226651733549215\n",
      "Gradient Descent(429/499): loss=107.09746238976629, w0=83.94037908387659, w1=5.108565521460827\n",
      "Gradient Descent(430/499): loss=51.33437425274227, w0=80.99354569874583, w1=9.928266574785694\n",
      "Gradient Descent(431/499): loss=19.97168537497264, w0=75.84034318047975, w1=11.840403374862804\n",
      "Gradient Descent(432/499): loss=31.147575578460255, w0=73.90046870207794, w1=19.061421538840115\n",
      "Gradient Descent(433/499): loss=37.380784870504705, w0=72.73365835923148, w1=6.87093806648285\n",
      "Gradient Descent(434/499): loss=78.68816658662593, w0=65.40041710425446, w1=21.498262045934872\n",
      "Gradient Descent(435/499): loss=33.157998300720145, w0=73.89181674910145, w1=19.41155386369361\n",
      "Gradient Descent(436/499): loss=33.485197959632856, w0=78.67118278063099, w1=10.780881817603987\n",
      "Gradient Descent(437/499): loss=74.0578018930863, w0=62.754993981259226, w1=10.97475251116958\n",
      "Gradient Descent(438/499): loss=71.96599582035552, w0=81.66137033280638, w1=6.911149096168461\n",
      "Gradient Descent(439/499): loss=97.82784410191873, w0=71.33581496611103, w1=26.170249433153025\n",
      "Gradient Descent(440/499): loss=62.70762544778364, w0=63.92953107430331, w1=10.843112785969414\n",
      "Gradient Descent(441/499): loss=56.3811116601971, w0=70.3051542595926, w1=4.932331844185125\n",
      "Gradient Descent(442/499): loss=47.94484830771216, w0=75.22544717662991, w1=5.644718409596132\n",
      "Gradient Descent(443/499): loss=18.676429868218484, w0=75.00450857978414, w1=11.567912874615953\n",
      "Gradient Descent(444/499): loss=78.59880418934237, w0=71.93183071012083, w1=2.3185951095002437\n",
      "Gradient Descent(445/499): loss=21.76730912830552, w0=72.04947250346272, w1=16.82847160284223\n",
      "Gradient Descent(446/499): loss=219.55508557091412, w0=57.83294302205887, w1=0.46831264931126704\n",
      "Gradient Descent(447/499): loss=27.779281580760948, w0=72.23865169319542, w1=18.34522289114846\n",
      "Gradient Descent(448/499): loss=2705.6359549685867, w0=31.30076248521374, w1=-46.66240831235764\n",
      "Gradient Descent(449/499): loss=21.265239484038815, w0=76.68771074979661, w1=13.970529270949218\n",
      "Gradient Descent(450/499): loss=50.440607374058224, w0=67.91257489120049, w1=19.894581278731312\n",
      "Gradient Descent(451/499): loss=823.6175478547998, w0=89.72417259684732, w1=50.17453748384189\n",
      "Gradient Descent(452/499): loss=224.2790286106136, w0=92.84669943058277, w1=7.52360831266331\n",
      "Gradient Descent(453/499): loss=103.76940056683003, w0=77.18979649696146, w1=0.767938864148138\n",
      "Gradient Descent(454/499): loss=666.2110040763522, w0=73.99053345656196, w1=-22.591951858422266\n",
      "Gradient Descent(455/499): loss=33.326132727642, w0=78.92905620549645, w1=15.510907213289533\n",
      "Gradient Descent(456/499): loss=28.745562016399553, w0=68.12514542978047, w1=13.535363495423464\n",
      "Gradient Descent(457/499): loss=147.92373575160104, w0=61.03596430991736, w1=24.195036460984536\n",
      "Gradient Descent(458/499): loss=34.65985797811154, w0=76.69933920157953, w1=8.288270061612595\n",
      "Gradient Descent(459/499): loss=41.0696430465428, w0=69.75560722658571, w1=19.712515889676776\n",
      "Gradient Descent(460/499): loss=861.9053513507565, w0=58.035117465629426, w1=51.69237801960221\n",
      "Gradient Descent(461/499): loss=208.91873744186466, w0=88.50064181858082, w1=0.996869346602594\n",
      "Gradient Descent(462/499): loss=17.550116505970234, w0=72.71505219124124, w1=15.478053501061606\n",
      "Gradient Descent(463/499): loss=19.686466507791955, w0=76.18699646633806, w1=12.998799321912683\n",
      "Gradient Descent(464/499): loss=124.58405539143386, w0=62.232548102437626, w1=23.279831949411403\n",
      "Gradient Descent(465/499): loss=66.80859134205647, w0=74.02760688663007, w1=23.59441032650681\n",
      "Gradient Descent(466/499): loss=27.132370000651775, w0=76.84317597598152, w1=10.178839748476015\n",
      "Gradient Descent(467/499): loss=314.05236384435744, w0=52.01415291900401, w1=25.500709886155356\n",
      "Gradient Descent(468/499): loss=3047.540984787326, w0=-4.576294313684578, w1=14.214290388036275\n",
      "Gradient Descent(469/499): loss=54.10613572209578, w0=66.21751643704269, w1=18.710867181819346\n",
      "Gradient Descent(470/499): loss=134.6535695943307, w0=79.13782921746449, w1=-0.816584704804665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(471/499): loss=175.07912967940624, w0=84.61055865399102, w1=-0.3521427735629672\n",
      "Gradient Descent(472/499): loss=57.950300207321355, w0=64.55121908707414, w1=16.428266388453615\n",
      "Gradient Descent(473/499): loss=113.4620469168426, w0=82.33839813788667, w1=2.7862672686590972\n",
      "Gradient Descent(474/499): loss=67.49142705423101, w0=63.42742705911269, w1=16.09951317372979\n",
      "Gradient Descent(475/499): loss=26.120218658674403, w0=71.08918297278201, w1=17.554977750556745\n",
      "Gradient Descent(476/499): loss=24.47242150242857, w0=69.04794858707166, w1=13.09921667150825\n",
      "Gradient Descent(477/499): loss=88.941799687302, w0=63.09003992704122, w1=20.036587775815136\n",
      "Gradient Descent(478/499): loss=1871.5052145808693, w0=59.17775843278418, w1=72.75004716922395\n",
      "Gradient Descent(479/499): loss=52.50626467951084, w0=79.58140222096446, w1=19.371091400801408\n",
      "Gradient Descent(480/499): loss=102.5479285778106, w0=70.042710129066, w1=26.276341005615425\n",
      "Gradient Descent(481/499): loss=58.784358706361765, w0=80.99157915623637, w1=18.727856489082637\n",
      "Gradient Descent(482/499): loss=81.04617341623197, w0=81.59073629331478, w1=21.38435940446122\n",
      "Gradient Descent(483/499): loss=107.52696706550927, w0=70.9573434104262, w1=26.852167664985785\n",
      "Gradient Descent(484/499): loss=623.825695005016, w0=94.85284286503246, w1=-13.944593308245564\n",
      "Gradient Descent(485/499): loss=512.6613410696177, w0=41.94881501012257, w1=16.948887273872046\n",
      "Gradient Descent(486/499): loss=30.280353981202527, w0=77.40125791391462, w1=17.073975075963812\n",
      "Gradient Descent(487/499): loss=667.473953896474, w0=45.12939337556232, w1=36.083593944586795\n",
      "Gradient Descent(488/499): loss=67.641463718032, w0=83.09173215193657, w1=10.561824848692014\n",
      "Gradient Descent(489/499): loss=113.13177068322804, w0=64.3111080666023, w1=2.765198006937137\n",
      "Gradient Descent(490/499): loss=287.15224291365473, w0=83.64559654391925, w1=-7.409891301559155\n",
      "Gradient Descent(491/499): loss=83.38153062314144, w0=77.39378778748238, w1=24.396780999879425\n",
      "Gradient Descent(492/499): loss=101.89942545010844, w0=76.49985323100225, w1=26.237027204098602\n",
      "Gradient Descent(493/499): loss=42.11667990624012, w0=67.33729291972801, w1=9.239411260567595\n",
      "Gradient Descent(494/499): loss=23.179284267804874, w0=73.4111984875423, w1=17.42598143543928\n",
      "Gradient Descent(495/499): loss=29.67317052782545, w0=76.36405936072966, w1=17.855649125249833\n",
      "Gradient Descent(496/499): loss=4204.1637056693535, w0=114.52030565529482, w1=95.19838182259639\n",
      "Gradient Descent(497/499): loss=1611.6232339495325, w0=39.05140863389677, w1=58.42328784226467\n",
      "Gradient Descent(498/499): loss=25.014890175336323, w0=70.70392132963211, w1=9.937128472737363\n",
      "Gradient Descent(499/499): loss=51.820424135008885, w0=73.93172674753082, w1=4.9672346735454695\n",
      "SGD: execution time=6.583 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "# Using 'stochastic_gradient_descent'\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "num_batches = 200\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, num_batches, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: When **outliers** occur in the dataset, because of measurement errors for instance, MSE (Mean Square Error), as implemented here above, is known to be sensitive to outliers. One way to deal with outliers is to use a more *robust* function, such as MAE (Mean Absolute Error). Nevertheless MAE is not differentiable and so we cannot use directly GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
